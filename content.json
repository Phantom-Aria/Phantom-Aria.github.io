{"meta":{"title":"我的小破站","subtitle":"","description":"","author":null,"url":"http://www.shelven.com","root":"/"},"pages":[{"title":"","date":"2022-04-09T14:14:36.602Z","updated":"2022-04-09T14:14:36.598Z","comments":true,"path":"404.html","permalink":"http://www.shelven.com/404.html","excerpt":"","text":"404 访问的页面走丢了！(⊙_⊙) 可能是输入地址有误或该地址已被删除 如果确认地址无误，请踢我一脚马上来改 TAT"},{"title":"","date":"2022-04-13T13:37:18.783Z","updated":"2022-04-13T13:37:18.770Z","comments":true,"path":"about/index.html","permalink":"http://www.shelven.com/about/index.html","excerpt":"","text":"感谢小伙伴的来访！ 小破站尚在搭建中，有好的想法请留言~ 本站成立的初衷是记录本人学习笔记，以及整合各种有用的生信网站和工具，方便查阅和学习。本人是前端小白指bug越修越多,如果本站有bug请留言，非常感谢!各位dalao高抬贵手，请不要DDOS攻击_(:з」∠)_"},{"title":"所有分类","date":"2022-04-13T09:37:32.168Z","updated":"2022-04-13T09:37:32.164Z","comments":true,"path":"categories/index.html","permalink":"http://www.shelven.com/categories/index.html","excerpt":"","text":"没有你感兴趣的？0.0 请留言提出你的宝贵意见~"},{"title":"","date":"2022-05-23T17:58:27.843Z","updated":"2022-05-23T17:58:27.841Z","comments":true,"path":"friends/index.html","permalink":"http://www.shelven.com/friends/index.html","excerpt":"生信网站快速导航 本页面暂时无法使用站内搜索 （绝对不是因为我懒地改代码） 快速查找需要的网站请使用 ctrl + F","text":"生信网站快速导航 本页面暂时无法使用站内搜索 （绝对不是因为我懒地改代码） 快速查找需要的网站请使用 ctrl + F 交换友链请参照下列格式并留言 名字|name: Phantom链接地址|link: http://www.shelven.com/头像地址|avatar: https://www.shelven.com/tuchuang/avatar.jpg描述|desc: 博学而笃志，切问而近思"},{"title":"","date":"2022-05-17T20:42:28.367Z","updated":"2022-05-17T20:42:28.363Z","comments":false,"path":"history/index.html","permalink":"http://www.shelven.com/history/index.html","excerpt":"","text":"2022-05-18 更新日志：新增栏目 新增网址导航栏目，优化导航栏 2022-05-16 更新日志 发表日志：转录组数据分析笔记（10）——初识GO&#x2F;KEGG富集分析 2022-05-08 更新日志 发表日志：视频一键转字符动画——python函数封装和调用练习 2022-05-04 更新日志 发表日志：转录组数据分析笔记（9）——pheatmap绘制差异表达基因热图 2022-05-04 更新日志 发表日志：获得测序原始数据——初探GEO和SRA数据库 2022-05-02 更新日志 发表日志：简易爬虫程序编程记录——以微博热搜为例 2022-04-29 更新日志：图床迁移 github图床已挂，2022年4月29日之后本站图床全部迁移本地 发表日志：vscode远程连接和快速搭建python环境 2022-04-25 更新日志 发表日志：转录组数据分析笔记（8）——ggplot2和ggrepel绘制火山图 2022-04-20 更新日志 发表日志：shell脚本基本语法总结 发表日志：linux操作指令总结整理 2022-04-19 更新日志：图床迁移 优化相册栏目 背景图片图床迁移至本地 2022-04-18 更新日志：新增栏目 发表日志：转录组数据分析笔记（7）——DESeq2差异分析 新增相册栏目 2022-04-17 更新日志 发表日志：转录组数据分析笔记（5）——stringtie转录本组装和定量 发表日志：转录组数据分析笔记（6）——HTseq计数定量 2022-04-16 更新日志：看板娘优化 看板娘模块显示优化 发表日志：转录组数据分析笔记（3）——samtools用法小结 发表日志：转录组数据分析笔记（4）——IGV基因组浏览器安装和解读 2022-04-15 更新日志：页面优化 发表日志：转录组数据分析笔记（2）——使用Hisat2比对参考基因组 网站页脚优化 2022-04-14 更新日志：bug修复 修复文章永久链接中文乱码bug，已更改永久链接格式 修复部分页面强制重新加载bug，修改了部分pjax代码 修复aplayer播放器切换页面后中断播放bug，同上 发表日志：转录组数据分析笔记（1）——如何用fastqc和trim-galore做测序数据质控 2022-04-13 更新日志：bug修复和CDN加速 修复主页轮播图片空白bug，修改了部分parallax代码，已优化图片加载方式 本站已加入又拍云联盟，站点已进行CDN加速 发表日志：小破站正式对外开放啦！ 2022-04-12 更新日志：服务器迁移 服务器迁移至本地 本站已绑定www.shelven.com域名 本站已安装SSL安全证书 2022-04-11 更新日志：界面优化 添加pjax插件，优化部分页面加载速度 百度统计维护，5月31日恢复接入 bug:暂时无法全站无刷新加载 2022-04-10 更新日志：功能添加 添加rss订阅功能 新增结绳栏目 新增站内搜索功能 接入百度统计 2022-04-09 更新日志：功能添加 新增live2D看板娘模块 接入腾讯云开发和twikoo评论系统 启用MFA，新增留言提醒和QQ邮箱头像抓取功能 2022-04-08 更新日志：页面美化 更改鼠标样式 更改侧边栏配置 新增网页加载条 2022-04-07 更新日志：页面美化和功能更新 重设导航栏 重设封面 更改右键菜单功能 2022-04-06 更新日志：功能添加 添加背景音乐功能，接入aplayer播放器 添加网站统计访客数功能，接入LeanCloud 添加文章字数统计功能 2022-04-05 更新日志：页面美化 导航栏设计和美化 卡片添加透明度 优化主题背景图片 2022-04-04 更新日志：小破站成立啦！ 使用Hexo框架，volantis主题搭建网站 github建库，代码托管 建立图床，使用jsDelivr进行CDN加速"},{"title":"","date":"2022-04-17T12:56:16.792Z","updated":"2022-04-17T12:56:16.792Z","comments":true,"path":"more/404.html","permalink":"http://www.shelven.com/more/404.html","excerpt":"","text":"404 访问的页面走丢了！(⊙_⊙) 可能是输入地址有误或该地址已被删除 如果确认地址无误，请踢我一脚马上来改 TAT"},{"title":"","date":"2022-04-13T07:44:09.805Z","updated":"2022-04-13T07:44:09.801Z","comments":true,"path":"mylist/index.html","permalink":"http://www.shelven.com/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-04-13T09:37:59.531Z","updated":"2022-04-13T09:37:59.527Z","comments":true,"path":"tags/index.html","permalink":"http://www.shelven.com/tags/index.html","excerpt":"","text":"没有你感兴趣的？0.0 请留言提出你的宝贵意见~"},{"title":"壁纸分享(点击查看大图)","date":"2022-04-19T06:40:53.941Z","updated":"2022-04-19T06:40:53.937Z","comments":true,"path":"photo/index.html","permalink":"http://www.shelven.com/photo/index.html","excerpt":"","text":""},{"title":"实时微博热搜榜","date":"2022-04-27T18:04:09.706Z","updated":"2022-04-27T18:04:09.701Z","comments":true,"path":"weibo/index.html","permalink":"http://www.shelven.com/weibo/index.html","excerpt":"","text":""}],"posts":[{"title":"转录组数据分析笔记（10）——初识GO/KEGG富集分析","slug":"转录组数据分析笔记（10）——初识GO-KEGG富集分析","date":"2022-05-15T19:47:39.000Z","updated":"2022-05-15T20:04:18.802Z","comments":true,"path":"2022/05/16/a.html","link":"","permalink":"http://www.shelven.com/2022/05/16/a.html","excerpt":"","text":"前言前面介绍了如何找到差异基因，我们通过R包DESeq2获得了差异表达基因，在此基础上做了更为直观的火山图和差异表达基因热图。但是仅仅知道差异表达基因的名字还不够，我们还要知道它到底有哪些功能和特征，就比如我看到一个很养眼的动漫角色，我就要去查查出自哪部番，是怎么样的人设和背景故事，一样的道理。 一个基因没有注释信息，那就只是一段核苷酸序列，有了注释信息我们才能知道这个基因在染色体上的定位，在具体的某个代谢途径上发挥什么功能等等。网上能找到很多注释信息的数据库，比如模式生物拟南芥TAIR，人类基因组hg19等等，Bioconductor有一个专门用来搜集注释信息数据库的工具包——AnnotationHub。 1. AnnotationHub注释数据库搜索工具用bioconductor下载Annotationhub包，载入（注：为演示结果，以下命令均在Rstudio终端输入） 123456789101112131415161718&gt; library(&quot;Annotationhub&quot;)&gt; hub &lt;- AnnotationHub()C:\\Users\\HUAWEI\\AppData\\Local/R/cache/R/AnnotationHub does not exist, create directory? (yes/no): yes |===================================================| 100%snapshotDate(): 2021-10-20&gt; hubAnnotationHub with 62386 records# snapshotDate(): 2021-10-20# $dataprovider: Ensembl, BroadInstitute, UCSC, ftp://ftp...# $species: Homo sapiens, Mus musculus, Drosophila melano...# $rdataclass: GRanges, TwoBitFile, BigWigFile, EnsDb, Rl...# additional mcols(): taxonomyid, genome,# description, coordinate_1_based, maintainer,# rdatadateadded, preparerclass, tags, rdatapath,# sourceurl, sourcetype # retrieve records with, e.g., &#x27;object[[&quot;AH5012&quot;]]&#x27; 第一次使用AnnotationHub需要创建一个AnnotationHub对象。为了更直观地使用，我们将AnnotationHub对象赋值给hub变量。查看这个变量，我们可以得到如下的信息。 数据库版本是2021-10-20，目前有62386条记录 可以用$dataprovider 方式查看数据来源，比如数据来自于Ensembl，UCSC等等 可以用$species 方式查看数据库有哪些物种，比如人类、小鼠等等 可以用$rdataclass 方式查看数据类型 可以通过函数mcols()查看更多信息 获取数据的方式是object[[&quot;AH5012&quot;]] object指你命名的变量名 以上就是AnnotationHub的标准用法，比如我想获得拟南芥的注释数据库，我就输入以下命令查找： 123456789101112131415161718192021222324&gt; query(hub, &quot;Arabidopsis thaliana&quot;)AnnotationHub with 13 records# snapshotDate(): 2021-10-20# $dataprovider: UCSC, PathBank, NCBI,DBCLS, FANTOM5,DLRP...# $species: Arabidopsis thaliana# $rdataclass: SQLiteFile, TxDb, Tibble, list, OrgDb, Inp...# additional mcols(): taxonomyid, genome,# description, coordinate_1_based, maintainer,# rdatadateadded, preparerclass, tags, rdatapath,# sourceurl, sourcetype # retrieve records with, e.g., &#x27;object[[&quot;AH10456&quot;]]&#x27; title AH10456 | hom.Arabidopsis_thaliana.inp8.sqlite AH52245 | TxDb.Athaliana.BioMart.plantsmart22.sqlite AH52246 | TxDb.Athaliana.BioMart.plantsmart25.sqlite AH52247 | TxDb.Athaliana.BioMart.plantsmart28.sqlite AH87070 | pathbank_Arabidopsis_thaliana_metabolites.rda ... ... AH91794 | wikipathways_Arabidopsis_thaliana_metabolites... AH95585 | Alternative Splicing Annotation for Arabidops... AH95951 | org.At.tair.db.sqlite AH97723 | LRBaseDb for Arabidopsis thaliana (Thale cres... AH97844 | MeSHDb for Arabidopsis thaliana (Thale cress,... query()函数查找，输入拟南芥的学名Arabidopsis thaliana我们可以看到一共找出了13个数据库。可以看到AH95951这个编号的数据库来源就是最大的拟南芥数据库TAIR（OrgDb，存储不同数据库基因ID之间对应关系，以及基因与GO等注释的对应关系，后面ID转换和GO分析要用到），我们就用这个数据库的注释资源。 1234&gt; hub[[&quot;AH95951&quot;]]downloading 1 resourcesretrieving 1 resource | | 0% 前面说过object[[&quot;AH5012&quot;]]是获取数据的方式。以上，就可以挂后台自动下载了。我这里因为网速的原因不下了，从前面的基因名也能看出来，我筛选的差异基因都是AT开头的，而且之前的基因组注释文件也是TAIR下载的，我可以直接用bioconductor安装org.At.tair.db包，这里用AnnotationHub只是提供一个找注释数据库的思路。 2. GO&#x2F;KEGG富集分析2.1 基因ID转换找到和下载注释数据库只是第一步，接下来GO&#x2F;KEGG富集分析需要用到R包clusterProfiler和org.At.tair.db 先来看一下我们基因名是什么格式的： 很明显，我们的基因ID是TAIR类型（废话，我从TAIR下的），org.At.tair.db包可以转换基因ID类型 可以用keytypes(org.At.tair.db)或者columns(org.At.tair.db)查看可以转换的基因ID类型 转换基因ID代码如下： 12345678library(&quot;org.At.tair.db&quot;)columns(org.At.tair.db) # 查看能转换基因的ID类型diffgen &lt;- nDEGs[, 1] # 注意只需要基因名diff_gen &lt;- bitr(diffgen, fromType = &quot;TAIR&quot;, toType = &quot;ENTREZID&quot;, # 基因ID类型TAIR转换为ENTREZID OrgDb = &quot;org.At.tair.db&quot;) # 该函数是基于org.At.tair.db包的diff_gen 这一步我的基因ID转换率只有60%左右，有将近一半的TAIR基因ID不能成功转换成ENTREZID，可能是Gene ID的版本问题，同一个基因在不同版本genecode中结果不一样，下载的注释文件原始版本我这里找不到了…暂时无法解决这个问题。只能不转换基因ID先跑一遍GO&#x2F;KEGG富集分析。 看了很多教程都说clusterProfiler需要的ID类型是ENTREZID，这里我持怀疑态度，不转换后续也能得到结果，我看了函数enrichGO()默认的基因ID是ENTREZID并不代表不能改变，有可能是误传。查阅了一些资料，简单来说Entrez ID是来自于NCBI旗下Entrez gene数据库的编号系统，基因编号系统之间是可以相互转换的，这些ID可以在对应的数据库找到基因注释信息，就是说也可以在网页上手动注释。 2.2 GO&#x2F;KEGG分析123456789101112131415161718192021library(&quot;clusterProfiler&quot;)# GO富集分析enrich_GO &lt;- enrichGO(gene = diffgen, # 基因名列表 OrgDb = &#x27;org.At.tair.db&#x27;, # 输入OrgDb数据库（注释对象信息） keyType = &#x27;TAIR&#x27;, # 输入的基因名ID类型 ont = &#x27;ALL&#x27;, # 输出的GO分类 pAdjustMethod = &#x27;fdr&#x27;, pvalueCutoff = 0.05, qvalueCutoff = 0.2, readable = TRUE)GO_result &lt;- enrich_GO@resultwrite.table(GO_result, &#x27;GO_result.csv&#x27;, sep = &#x27;,&#x27;, quote = FALSE, row.names = FALSE)# KEGG富集分析enrich_KEGG &lt;- enrichKEGG(gene = diffgen, keyType = &quot;kegg&quot;, organism = &quot;ath&quot;, # 输入的物种名 pvalueCutoff = 0.05, qvalueCutoff = 0.2)KEGG_result &lt;- enrich_KEGG@resultwrite.table(KEGG_result, &#x27;KEGG_result.csv&#x27;, sep = &#x27;,&#x27;, quote = FALSE, row.names = FALSE) clusterProfiler这个包进行GO和KEGG富集分析就这两个函数 这里我的GO只富集到两条细胞组分的内容： 说一下各列代表的意思： ONTOLOGY GO分类BP（生物学过程）、CC（细胞组分）或MF（分子功能） ID 富集到的GO id号 Description 富集到的GO描述 GeneRatio和BgRatio 分别为富集到该GO条目中的基因数目&#x2F;给定基因的总数目，以及该条目中背景基因总数目&#x2F;该物种所有已知的GO功能基因数目 pvalue、p.adjust和qvalue p值、校正后p值和q值信息 geneID和Count，富集到该GO条目中的基因名称和数目 KEGG富集分析结果表如下： ID和Description 分别代表富集到KEGG的ID和描述，其他和GO富集都类似 KEGG富集分析的时候有一点需要注意，输入的organism名称需要在官网的KEGG Organisms列表中能找到，否则是不能进行分析的！点击这里进入KEGG Organisms: Complete Genomes 还发现一个很奇怪的问题，我在官网的Organisms列表能找到拟南芥Arabidopsis thaliana，但是在上面的函数中对参数赋值organism = &quot;Arabidopsis thaliana&quot;会显示HTTP 400错误，也就是发出的url请求有问题，但是输入organism = &quot;ath&quot;程序可以正常运行，以后注意写缩写吧（应该是只有缩写才行，会通过联网自动获取该物种的pathway注释信息）。 3. 可视化clusterProfiler包还提供了GO&#x2F;KEGG富集结果的可视化方案，此处代码参考CSDN，作者：Tian問 这里因为我的GO结果不好，只简单写一下流程，详细作图函数参数使用方法和效果同样可以参考上面的链接~ 12345678910111213141516171819202122232425## GO富集分析可视化#barplotbarplot(enrich_GO, showCategory = 10)#dotplotdotplot(enrich_GO, showCategory = 10)#DAG有向无环图plotGOgraph(enrich_GO) #矩形代表富集到的top10个GO terms, 颜色从黄色过滤到红色，对应p值从大到小。#igraph布局的DAGgoplot(enrich_GO)#GO terms关系网络图（通过差异基因关联）emapplot(enrich_GO, showCategory = 30)#GO term与差异基因关系网络图cnetplot(enrich_GO, showCategory = 5)## KEGG富集分析可视化#barplotbarplot(enrich_KEGG, showCategory = 10)#dotplotdotplot(enrich_KEGG, showCategory = 10)#pathway关系网络图（通过差异基因关联）emapplot(enrich_KEGG, showCategory = 30)#pathway与差异基因关系网络图cnetplot(enrich_KEGG, showCategory = 5)#pathway映射browseKEGG(enrich_KEGG, &quot;ath03060&quot;) #在pathway通路图上标记富集到的基因，会弹出页面链接到KEGG官网 关于GO&#x2F;KEGG富集分析，还有非常多的操作和应用，我只是简单做个最基础的富集分析的学习，没有涉及到手动注释，构建orgdb等等更多操作。电脑快没电了，这篇笔记先暂时记这些，以后需要用到再补充~","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"AnnotationHub","slug":"AnnotationHub","permalink":"http://www.shelven.com/tags/AnnotationHub/"},{"name":"GO/KEGG","slug":"GO-KEGG","permalink":"http://www.shelven.com/tags/GO-KEGG/"},{"name":"org.At.tair.db","slug":"org-At-tair-db","permalink":"http://www.shelven.com/tags/org-At-tair-db/"}]},{"title":"视频一键转字符动画——python函数封装和调用练习","slug":"视频一键转字符动画——python函数封装和调用练习","date":"2022-05-07T17:43:17.000Z","updated":"2022-05-07T18:35:00.862Z","comments":true,"path":"2022/05/08/a.html","link":"","permalink":"http://www.shelven.com/2022/05/08/a.html","excerpt":"","text":"1. 前言捣鼓了几天python代码，我现在也越来越发现python的魅力所在，它的强大之处在于有非常多的第三方库可以随意调用。我不需要知道这些第三方库各种函数的实现方式，只要知道这些函数有什么作用，能得到什么结果。只要构思好自己的想法，找到对应的库就可以一步步按照我的思路编写程序，实现我想要的结果，整个构思到实现的过程让我非常愉悦~ 写这篇博客纯粹是个人爱好，也是一个巧合~ 前几天刷b站看到有人做了个剪影的字符动画，我就很好奇python是否可以实现。参考了一下github上大佬们的图片转字符画的代码，对这些代码做了点深入研究，总算搞明白了其实现方式，并且自己动手修改代码，在原有基础上改了几个bug，新增几个模块的调用，最后一步封装写成了下面这个脚本。这个脚本的功能是只要输入视频文件和你想要的视频帧率，就可以自动将视频转化为字符动画。 可以先看一下视频效果~ 或者点击这里进入b站观看 Your browser does not support the video tag. 2. 实现思路 调用ffmpeg根据帧率将视频切割成图片 调用pillow库作图，每张图片转换字符画 调用ffmpeg合并字符画并输出动画 3. 脚本代码及详解思路很清晰，接下来是写代码实现的过程，细节方面需要调用其他库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182from PIL import Image, ImageDraw, ImageFont # pillow库作图import subprocess # 执行命令行命令，为了调用ffmpegimport sysimport os # 操作目录用import shutil # 删除目录用import numpy as np # 转化numpy数组import gc # 优化运行内存用到# 定义输入值，这个脚本需要两个输入值：file_input和FPSfile_input = sys.argv[1]FPS = sys.argv[2]def do_turn(file_input, FPS): # 调用ffmpeg切割视频 os.makedirs(&quot;tempfile/cut/&quot;) # 当前目录新建存放切割图片的临时文件夹 shell_vedio = &quot;ffmpeg -i &quot; + file_input + &quot; -r &quot; + FPS + &quot; -qscale:v 2 ./tempfile/cut/%05d.jpg&quot; # 按照XXXXX序号切割 shell_voice = &quot;ffmpeg -i &quot; + file_input + &quot; ./tempfile/out.mp3&quot; subprocess.call(shell_vedio, shell=True) # 切割视频 subprocess.call(shell_voice, shell=True) # 分离音频 count =0 for file in os.listdir(&quot;./tempfile/cut/&quot;): count += 1 print(&quot;成功分离音频，截图开始转换字符画......&quot; + &quot;共计&quot; + str(count) + &quot;张&quot;) # 统计切割图张数 # 图片转换字符画 list_p = os.listdir(&quot;./tempfile/cut/&quot;) cwd = os.getcwd() os.mkdir(&quot;./tempfile/new/&quot;) # 新建存放字符画的临时文件夹 process = 1 # 统计完成转换的字符画数量 for id in list_p: # 遍历cut文件夹所有切割后的图片做字符画转换 address = str(&quot;&quot;.join(cwd + &#x27;/tempfile/cut/&#x27; + id)) # 拼接文件的绝对路径 im = Image.open(address) # 调用image打开图片 font = ImageFont.truetype(&quot;DejaVuSans-Bold&quot;, size=20) # 字体模式，可更改 rate = 0.1 # 缩放比（不调整的话像素点过多，这里统一调整） aspect_ratio = font.getsize(&quot;x&quot;)[0] / font.getsize(&quot;x&quot;)[1] # 获得字符长宽比 new_im_size = np.array([im.size[0] * rate, im.size[1] * rate * aspect_ratio]).astype(int) # 转换numpy数组，调整大小 im = im.resize(new_im_size) im = np.array(im.convert(&quot;L&quot;)) # 转换灰阶图，生成numpy数组 symbols = np.array(list(&quot; .-vM@&quot;)) # 建立字符索引，注意要按照亮度手动排序 if im.max() == im.min(): # 全黑和全是一种颜色进行区分 if im.max() &gt; 0: # 全是一种颜色，亮度值大于0，则全部用最亮的字符数值 im = (im / im) * (symbols.size - 1) else: im[np.isnan(im)] = 0 # 全黑时亮度值为NaN(非数值)，则全部用最暗字符的字符数值，也就是全黑 else: im = (im - im.min()) / (im.max() - im.min()) * (symbols.size - 1) # 根据索引赋予相应像素点相应的数值 ascii = symbols[im.astype(int)] letter_size = font.getsize(&quot;x&quot;) # 获取字符大小，与前面一定要对应 im_out_size = new_im_size * letter_size # 这里乘以字符长宽，否则字符只有一个像素点大小 im_out = Image.new(&quot;RGB&quot;, tuple(im_out_size), &quot;black&quot;) # 设置输出图片，背景 draw = ImageDraw.Draw(im_out) y = 0 # 两个循环穷举赋值，做字符画图片 for i, m in enumerate(ascii): for j, n in enumerate(m): draw.text((letter_size[0] * j, y), n, font=font) y += letter_size[1] # 注意+=，这里赋值字符宽度给y值 im_out.save(&quot;./tempfile/new/&quot; + id + &quot;.png&quot;) # 定义输出位置和图片格式 print(address + &quot;转换成功！当前进度：&quot; + str(process) + &quot;/&quot; + str(count)) # 显示进度 process += 1 gc.collect() # 重要！每次循环结束释放一次内存，否则容易内存溢出 print(&quot;转换成功！开始生成视频，请稍候......&quot;) # 调用ffmpeg合并字符画为视频，并且合并分离的音频 outvedio = &quot;ffmpeg -r &quot; + FPS + &quot; -i ./tempfile/new/%05d.jpg.png ./tempfile/out.mp4&quot; subprocess.call(outvedio, shell=True) final_vedio = &quot;ffmpeg -i ./tempfile/out.mp4 -i ./tempfile/out.mp3 final.mp4&quot; subprocess.call(final_vedio, shell=True) shutil.rmtree(&quot;./tempfile&quot;) # 删除临时文件夹 print(&quot;字符动画final.mp4已生成!已移除临时文件夹&quot;)# 输入格式错误则显示该条用法def usage(): print(&quot;usage:&quot;, sys.argv[0], &quot;&lt;file_input&gt; &lt;FPS&gt;&quot;) exit(0)if __name__ == &quot;__main__&quot;: # 封装，只有在文件作为脚本直接执行时后面的语句才会被执行，而 import 到其他脚本中后面的语句是不会被执行的 if len(sys.argv) != 3: # 判断输入的值是否为两个，没错，是判断两个 usage() else: do_turn(file_input, FPS) 4. 注意要点调用numpy模块生成数组，是因为python本身虽然可以建立多维度的数组，但是书写起来非常麻烦。numpy可以很好地解决这个问题，可以理解为能构建一个更好用的数组。在对数组进行遍历穷举，要注意两次穷举分别生成两个数组，第二次生成的数组只有一个数，所以下面draw.text第二个参数text可以用n，也可以用n[0]列举第一个数。 12345y = 0 for i, m in enumerate(ascii): # 这里i是用不到的 for j, n in enumerate(m): draw.text((letter_size[0] * j, y), n, font=font) # 第一个参数是确定坐标 y += letter_size[1] 字符大小，字符格式，以什么字符为参照，都是可以调整的。只要注意一点，我们是按照像素点的亮度来赋于这个像素点用什么字符的，所以索引列比较重要，要自己按照字符亮度排序，添加字符注意改值。 1symbols = np.array(list(&quot; .-vM@&quot;)) # 可以改成自己想要用的字符，注意按照亮度升序 还有，在计算亮度和赋予索引值的时候，我们是按照相对亮度来计算的。因此，当图片所有像素点都是一种颜色的时候，im.max() 和 im.min()值是相等的，相对亮度会出现0&#x2F;0的值，导致报错。所以我加了以下判断条件：纯色黑色和其他颜色属于两种不同情况，黑色时numpy数组亮度是非数值NaN，需要将数组全部值进行替换为亮度最小的字符的值；因为是RGB取值，其他颜色值固定在0-255之间，颜色均一，相对亮度就没有意义了，因此全部调整为最亮字符的值。 1234567if im.max() == im.min(): if im.max() &gt; 0: im = (im / im) * (symbols.size - 1) else: im[np.isnan(im)] = 0else: im = (im - im.min()) / (im.max() - im.min()) * (symbols.size - 1) 顺便再说一个很有意思的模块subprocess，subprocess.call()函数可以执行命令行的命令，并且这个命令是在子进程实行的，只有子程序结束才会继续执行其他命令，使用起来真的特别方便！比如有些程序我的python库里没有但是我的linux里有，在python脚本的某一步我需要用到linux里的软件去处理，这个时候就可以调用subprocess.call()函数去执行linux命令行的命令了。 还有一个函数虽然不显眼，但是起着至关重要的作用 gc.collect() 没有这个函数部分运行内存不够的电脑会崩……我在这里踩了个大坑…… 在对程序进行简化以后，我以为优化地差不多了，然后发现有的时候程序会被莫名其妙killed…… vi /var/log/messages 查看运行日志，好家伙，内存溢出了 经过一番度娘，我检查了一下自己也没有用到循环引用的变量啊，那么真相只有一个了：转换字符画部分程序有3个for循环嵌套，可能是for循环引用的对象没有及时回收导致内存不断增长，最后被系统kill掉（个人猜测）。虽然python本身有垃圾回收功能，而在程序运行的时候清理地并不是很及时，引入的gc模块是python的垃圾收集器模块，与默认的回收方式算法不同，gc.collect()函数可以强制进行垃圾回收。因此我在每个转化字符画的for循环执行一次结束后强制回收内存，如下： 效果是立竿见影的，内存占用再也没超过5%了，非常的稳定！ 5. 食用方法缺什么第三方库就装什么，主要是pillow库、numpy库和ffmpeg，用conda可以直接安装。上面那段脚本代码复制粘贴，保存为ascii.py，运行命令： python ascii.py &lt;视频文件&gt; &lt;你想要的视频帧率&gt; 回车，OK，静静等屏幕上的提示就好了。视频文件不在当前文件夹的话自行加上绝对路径，完成以后只会在当前目录生成一个out.mp4的输出文件。 源代码将同步上传我的github。","categories":[{"name":"编程自学","slug":"编程自学","permalink":"http://www.shelven.com/categories/%E7%BC%96%E7%A8%8B%E8%87%AA%E5%AD%A6/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.shelven.com/tags/python/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://www.shelven.com/tags/ffmpeg/"},{"name":"numpy","slug":"numpy","permalink":"http://www.shelven.com/tags/numpy/"},{"name":"pillow","slug":"pillow","permalink":"http://www.shelven.com/tags/pillow/"}]},{"title":"转录组数据分析笔记（9）——pheatmap绘制差异表达基因热图","slug":"转录组数据分析笔记（9）——pheatmap绘制差异表达基因热图","date":"2022-05-04T17:16:14.000Z","updated":"2022-05-04T17:40:38.589Z","comments":true,"path":"2022/05/05/a.html","link":"","permalink":"http://www.shelven.com/2022/05/05/a.html","excerpt":"","text":"1. 前言前面说到怎么用ggplot做一个火山图来查看各个基因的表达情况，火山图是以log2FC值为横坐标，以-log10（FDR）值作为纵坐标，将所有的基因都做了点状图。虽然能比较直观地看到所有基因表达情况，但我们真正感兴趣的是处理后差异表达的基因。因此，我们也可以通过前面得到的表达矩阵获得差异表达的基因名，对raw count数据进行提取和均一化，然后做一个差异基因的热图，能更直观地看到差异基因在各个样本中的上调下调情况。 做热图我们用的最多的R包是pheatmap，可以直接用biocmanager下载。 后面所有Rstudio操作都是在同一个Rproject中进行，引用的变量如果不理解就翻前面的笔记，最后我会把一整个转录组下游分析的R流程代码写成一个文件上传到github备份。 2. 热图介绍废话不多说，先上一段热图的定义介绍：热图是用来对采集的因子响应强度或其他的一些因素进行均一化，从而利用颜色条的变化来直观地表示不同样本之间的含量变化情况的图。 定义很简单，这里我们的因子响应强度就是每个基因的raw count值，但是raw count值从0到几千上万差别非常之大，作图不方便。所以我们通常会用均一化的方法，使每个基因的raw count值变化程度处于同一个数量级，再通过不同颜色变化得到基因在不同样品的含量变化。 R自带的均一化函数是scale()，注意下scale默认的均一化方式是按列进行的，我们还可以通过函数 t() 进行矩阵的行列转化，只需要将差异基因挑出来按行（也就是基因名）进行均一化，导入pheatmap包即可做成一个最简单的热图。 3. 简化版代码先上一个最最简易版的，比如我要分析前25个最可能发生差异表达的基因，代码如下： 1234567library(&quot;pheatmap&quot;)nDEGs &lt;- gene[which(gene$group != &quot;NOT_CHANGE&quot;),] # 筛选差异基因sort_DEGs &lt;- arrange(nDEGs,padj) # 按照padj值升序排序choose_gene &lt;- head(sort_DEGs[,1],25) # 取padj值最小的前25个基因choose_matrix &lt;- mycounts[choose_gene,] # 从raw count矩阵中挑出这25个基因数据heat_matrix &lt;- t(scale(t(choose_matrix))) # 转换了两次行列并均一化，实际就是按row进行了均一化pheatmap(heat_matrix) # 以默认参数做热图 在plots窗口可以预览生成的热图： 因为没有加任何参数调整，所以不好看（已经比R自带的热图函数做出来的好看了），先解释一下上面代码实现的原理。gene是我们前面做火山图的矩阵，里面已经有了我们差异基因分组的一列group nDEGs &lt;- gene[which(gene$group != &quot;NOT_CHANGE&quot;),] 这个代码是将group列中字符不等于“NOT_CHANGE”的数据挑出来赋值给nDEGs，注意下赋值后的nDEGs也是矩阵，可以直接查看。 sort_DEGs &lt;- arrange(nDEGs,padj) arrange() 函数的功能是升序排列，这里按照padj值升序排列。 choose_gene &lt;- head(sort_DEGs[,1],25) head()函数用法不说了，取了前25个基因。注意下我们取的第一列是基因名，如果你前面已经将基因名作为rownames导入了，那就要用rowname。 choose_matrix &lt;- mycounts[choose_gene,]，返回到我们前面的raw count矩阵，将基因名对应的数据挑出来，可以看下这个时候的choose_matrix矩阵是怎么样的： heat_matrix &lt;- t(scale(t(choose_matrix))) 先进行一次行列转换，对列数据进行均一化，再进行一次行列转换，说白了就是对每行基因的raw count数据进行均一化，得到如下矩阵： pheatmap(heat_matrix)以默认参数做热图，大功告成。 如果要对所有差异表达的基因做热图，只需要修改一下输入的矩阵就行： 123all_matrix &lt;- mycounts[(sort_DEGs[,1]),]heat_matrix_all &lt;- t(scale(t(all_matrix)))pheatmap(heat_matrix_all) 因为这是默认参数作图，所以输出结果非常感人： 你论文敢用这种图？所以还是需要了解一下pheatmap包的各种参数，对热图进行调整和修改。 4. 参数详解此部分内容参考CSDN博客：跳动的喵尾巴 12345678910111213141516171819pheatmap(mat, color = colorRampPalette(rev(brewer.pal(n = 7, name = &quot;RdYlBu&quot;)))(100), kmeans_k = NA, breaks = NA, border_color = &quot;grey60&quot;, cellwidth = NA, cellheight = NA, scale = &quot;none&quot;, cluster_rows = TRUE, cluster_cols = TRUE, clustering_distance_rows = &quot;euclidean&quot;, clustering_distance_cols = &quot;euclidean&quot;, clustering_method = &quot;complete&quot;, clustering_callback = identity2, cutree_rows = NA, cutree_cols = NA, treeheight_row = ifelse((class(cluster_rows) == &quot;hclust&quot;) || cluster_rows, 50, 0), treeheight_col = ifelse((class(cluster_cols) == &quot;hclust&quot;) || cluster_cols, 50, 0), legend = TRUE, legend_breaks = NA, legend_labels = NA, annotation_row = NA, annotation_col = NA, annotation = NA, annotation_colors = NA, annotation_legend = TRUE, annotation_names_row = TRUE, annotation_names_col = TRUE, drop_levels = TRUE, show_rownames = T, show_colnames = T, main = NA, fontsize = 10, fontsize_row = fontsize, fontsize_col = fontsize, angle_col = c(&quot;270&quot;, &quot;0&quot;, &quot;45&quot;, &quot;90&quot;, &quot;315&quot;), display_numbers = F, number_format = &quot;%.2f&quot;, number_color = &quot;grey30&quot;, fontsize_number = 0.8 * fontsize, gaps_row = NULL, gaps_col = NULL, labels_row = NULL, labels_col = NULL, filename = NA, width = NA, height = NA, silent = FALSE, na_col = &quot;#DDDDDD&quot;, ...) 参数内容非常之多，我这里仅挑选一些可能用得上的做个记录： 参数 描述 color 表示热图颜色，colorRampPalette(rev(brewer.pal(n &#x3D; 7, name &#x3D; “RdYlBu”)))(100)表示颜色渐变调色板，“n” 的数量取决于调色板中颜色的数量，“name” 为调色板的名称，(100)表示100个等级；color &#x3D; colorRampPalette(c(“blue”, “white”, “red”))(100)则是通过设置三种不同的颜色进行渐变显示 scale 表示进行均一化的方向，值为 “row”, “column” 或者”none” kmeans_k 默认为NA，即不会对行进行聚类；如果想在进行层次聚类之前，先对行特征(因子)进行 k-means 聚类，则可在此调整热图的行聚类数 cluster_rows 表示仅对行聚类，值为TRUE或FALSE cluster_cols 表示仅对列聚类，值为TRUE或FALSE clustering_distance_cols 表示列聚类使用的度量方法，与行聚类的度量方法一致 clustering_method 表示聚类方法，包括：‘ward’, ‘ward.D’, ‘ward.D2’, ‘single’, ‘complete’, ‘average’, ‘mcquitty’, ‘median’, ‘centroid’ cutree_rows 若进行了行聚类，根据行聚类数量分隔热图行 cutree_cols 若进行了列聚类，根据列聚类数量分隔热图列 treeheight_row 若进行了行聚类，其热图行的聚类树高度，默认为 “50” treeheight_col 若进行了列聚类，其热图列的聚类树高度，默认为 “50” breaks 用来定义数值和颜色的对应关系，默认为 “NA” border_color 表示热图每个小的单元格边框的颜色，默认为 “NA” cellwidth 表示单个单元格的宽度，默认为 “NA”，即根据窗口自动调整 cellheight 表示单个单元格的高度，默认为 “NA”，即根据窗口自动调整 fontsize 表示热图中字体大小 fontsize_row 表示行名字体大小，默认与fontsize一致 fontsize_col 表示列名字体大小，默认与fontsize一致 fontsize_number 表示热图上显示数字的字体大小 angle_col 表示列标签的角度，可选择 “0”，“45”，“90”，“270”，“315” display_numbers 表示是否在单元格上显示原始数值或按照特殊条件进行区分标记 number_format 表示热图单元格上显示的数据格式，如 “%.2f” 表示两位小数； “%.1e” 表示科学计数法 number_color 表示热图单元格上显示的数据字体颜色 legend 表示是否显示图例，值为TRUE或FALSE annotation_row 表示是否对行进行注释 annotation_col 表示是否对列进行注释 annotation_colors 表示行注释及列注释的颜色 annotation_legend 表示是否显示注释的图例信息 annotation_names_row 表示是否显示行注释的名称 annotation_names_col 表示是否显示列注释的名称 show_rownames 表示是否显示行名 show_colnames 表示是否显示列名 main 表示热图的标题名字 gaps_row 仅在未进行行聚类时使用，表示在行方向上热图的隔断位置，如 gaps_row &#x3D; c(２, 4)表示在第２与第4列进行隔断 gaps_col 仅在未进行列聚类时使用，表示在列方向上热图的隔断位置，同 gaps_row labels_row 表示使用行标签代替行名 labels_col 表示使用列标签代替列名 filename 表示保存图片的位置及命名 width 表示输出绘制热图的宽度 height 表示输出绘制热图的高度 margins 表示热图距画布的空白距离 好吧，看上去基本上都能用到。制作热图应用的统计学原理就不多说了，我也没研究明白，我们用上面的这些参数能做出好看点的图就行。所以其实pheatmap中也有对应的参数scale来对行或列进行均一化，在pheatmap中设置参数或者多加一行代码做个均一化转换都是一样的。 5. 最终流程代码知道了简化版代码的各行命令和pheatmap各参数作用，稍加一点点修改得到最终流程代码： 12345678910111213141516library(&quot;pheatmap&quot;)nDEGs &lt;- gene[which(gene$group != &quot;NOT_CHANGE&quot;),]sort_DEGs &lt;- arrange(nDEGs,padj)all_matrix &lt;- mycounts[(sort_DEGs[,1]),]heat_matrix_all &lt;- t(scale(t(all_matrix)))# 要做热图的每列分组，底下两行代码是必须的annotation_col &lt;- data.frame(sample = factor(c(rep(&quot;SD&quot;,4),rep(&quot;LD1&quot;,4)))) # 简单粗暴地写一个分组矩阵row.names(annotation_col) &lt;- colnames(heat_matrix_all)pheatmap(heat_matrix_all, color = colorRampPalette(c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;))(100), treeheight_col = 30, treeheight_row = 10, show_rownames = FALSE, angle_col = 45, annotation_col = annotation_col, filename = &quot;test.pdf&quot;) 保存后的差异基因热图如上所示，左边4列是LD长日照1天组，右边4列是SD短日照对照组。数据不是特别好，但是两组还是能区分开的，至少也比第一次做的顺眼一点了。 标题参数main有个小bug，加了参数在plots区域预览是正常的，但是用filename保存到输出文件如果标题是中文会出错，无法显示中文标题。这个小bug倒是无伤大雅，图在plots区也可以直接保存输出，不是非得要用filname参数指定输出文件输出的。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://www.shelven.com/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"pheatmap","slug":"pheatmap","permalink":"http://www.shelven.com/tags/pheatmap/"}]},{"title":"获得测序原始数据——初探GEO和SRA数据库","slug":"获得测序原始数据——初探GEO和SRA数据库","date":"2022-05-03T20:02:37.000Z","updated":"2022-05-03T20:26:55.722Z","comments":true,"path":"2022/05/04/a.html","link":"","permalink":"http://www.shelven.com/2022/05/04/a.html","excerpt":"","text":"初探GEO数据库和SRA数据库最近在看转录组数据分析的文献，想下载一些原始数据自己跑一跑的，发现自己对于几个高通量测序数据库还是有些不太熟悉。以我现在的经验来看，EBI数据库的原始测序数据最容易获得，可以直接在EBI官网下载需要的fastq格式文件，但是NCBI的SRA数据库下载数据还是有些麻烦的，做个学习笔记记录下。 1. GEO数据库先上数据库链接，点击这里 GEO数据库全称Gene Expression Omnibus database，是由美国国立生物技术信息中心NCBI创建并维护的基因表达数据库。它创建于2000年，收录了世界各国研究机构提交的大多数高通量基因表达数据，GEO除了二代测序数据，还包含芯片测序、单细胞测序数据。 GEO数据库有四种数据存放类型**GSE数据编号(Series)、GPL数据编号(GEO platforms)、GSM数据编号(Samples)和GDS数据编号(Datasets)**。 一篇文章可以有一个或者多个GSE(Series)数据集，一个GSE里面可以有一个或者多个GSM(Samples)样本，如果做的是基因芯片，那每个数据集也会有自己对应的芯片平台，就是GPL(GEO platforms)。GSE编号一般为作者提交时生成的原始数据编号，后续NCBI中的工作人员会根据研究目的、样品类型等信息归纳整合为一个GDS(Datasets)，整理后的数据还会有GEO profile数据，也就是基因在这次实验中的表达数据。我平常文献里看到最多的就是GSE编号，可以直接在GEO数据库的搜索框里输入查看。 我个人比较关注的是tools栏目里的FTP Site功能。ftp是文件传输协议，ftp访问的界面非常干净，如果我只需要下载GEO数据库里的文件，而不关注其他多余信息的话，可以通过这个界面非常快速地找到对应GSE编号下所有作者上传到GEO数据库的文件，目录结构层次一目了然。如下图所示，datasets对应GDS编号；platforms对应GPL编号；samples对应GSM编号；series对应GSE编号，网站还很贴心地给了README.txt里面写了帮助文档，包括怎么用ftp访问等等 在Browse Content这个栏目里，我们也能看到这个数据库的一些概览，比如现在有多少数据集，多少个芯片平台，多少上传的样品等等信息。我翻看了一下两年前jimmy出的教程，那个时候的GEO数据库收录的信息只有现在的一半不到，两年时间这些数据呈井喷式的发展，大数据时代信息发展之快也确实让我挺震惊。 尤其是点开基因芯片的平台，以上传的样品数量对这些芯片平台进行排序，发现两年前独占鳌头的Affymetrix公司的HG-U133（也是最古老的基因芯片）已经被illumina公司高通量测序芯片全面超越了接近两倍，这也意味着高通量测序在这两年发展已经不仅仅是快速发展了，简直是火箭式发展……这也是大势所趋。 GEO数据库的搜索地址也很有规律性，比如只有最后的GSE编号不同，其他网址字段都是一模一样的，这也算是方便搜索的一个没什么用处的小技巧？…… 看点有用的，我们根据文献得到的GSE编号进行搜索，可以看到如下页面，前半部分是作者的一些信息，我们可以获得测序物种、作者的联系方式、发表的文章PMID等等，而我们更需要获取的信息在后半部分： 样本信息这里解释一下，有三种主要的数据类型： SOFT 平台信息芯片中探针与基因的对应关系注释文件，样品单独的表达量，所有信息文件 MINiML XML格式的所有数据，同SOFT文件单格式不同，和HTML格式差不多 TXT 这是样品表达矩阵的数据文件，我的理解是总结类型文件，不如用底下的原始数据 原始数据信息也说一下，这里可能会给原始测序的raw data，也可能不给；有的时候会给raw count表达矩阵，也有的时候不给；样品表达矩阵也是有的时候给的raw count，有的时候给的FPKM（比如上面这种情况）。就…挺离谱的，没有标准，如果我们要跑Deseq2复现作者的结论，通过FPKM得到的表达矩阵还不能用。希望网站能制定点相关标准吧（题外话，不然对于我们这种小白，找不到数据直接痛苦面具） 我个人觉得，这个数据库比较重要的是可以获得样品分组信息和SRA数据库的链接地址，方便我们下载测序原始数据（后面介绍怎么下载）。如果不想跑原始数据，也可以直接拿样品表达矩阵来跑转录组下游分析。直接用表达矩阵就是要当心作者拿漂亮数据坑你。 2. SRA数据库同样的先上数据库链接，点这里 其实从网址上就可以看出端倪，这俩数据库都是NCBI旗下的俩兄弟，都是NCBI一个亲爹亲妈养的。我百度的时候也发现，很多人把这两个数据库混在一起，或者叫GEO&#x2F;SRA数据库，其实两者还是有区别的。 SRA数据库是三大核酸数据库之一，我之前的笔记也有介绍过（点击这里查看）。我个人的理解是，SRA数据库存放的是原始测序文件，而GEO数据库存放的大部分是经过作者处理以后的数据文件（有的也包括了原始测序文件），相对而言SRA数据库更大也更存粹，而NCBI官方也给了下载SRA数据的小工具——SRA Toolkit。 SRA数据类型包含如下四种，看到前缀知道这是SRA数据库，了解一下就行： Studies 研究课题（前缀为ERP或SRP，包含多个Experiment) Experiments 实验设计（前缀为SRS，包含Sample、DNA source、测序平台和测序数据等信息） Samples 样品信息（前缀为SRX，包含一个或多个Runs) Runs 测序结果集（SSR开头的记录，代表测序仪器所产生的reads） 主要说说数据下载方式前几天也有同学问我sra数据库的原始测序数据怎么下载的，找不到下载方式，看的教程都是NCBI上直接下载的。emmmmmm我的第一反应是这不就是NCBI旗下的子数据库嘛……还能从哪儿下载… 废话不多说，直接看官网给的下载工具——SRA Toolkit 这是一个官方给的小工具合集，提供我们各种操作系统下的安装包，我把linux和windows安装包都下了。安装步骤都是一样的，解压，把bin文件夹路径加到系统环境变量，搞定。windows需要打开cmd命令行运行一次prefetch（下载命令），按照提示输入vdb-config --interactive起到类似激活的作用就行了。linux里甚至都不需要编译（也可以conda安装）。可能有的同学对自己的windows系统不熟悉，不知道怎么改自己的系统环境变量，其实这个比linux改环境变量更容易，百度一下吧&#x3D; &#x3D; 我个人更推荐在windows系统安装SRA Toolkit，SRA数据库本身服务器在国外，国内访问下载速度慢到令人发指（个位数Kb&#x2F;s，甚至直接没有），而类似转录组这种数据，细菌可能还好点，动植物做个10X测序动不动就是几个G十几个G，加上分组和生物学重复动辄几十上百G，那点速度下到天荒地老也下不完。纯命令行的linux系统使用代理服务相对windows系统来说要麻烦一点，说白了，windows系统更容易科学上网，为了下载数据没有别的办法。 我们用的就是SRA Toolkit工具包里的prefetch命令下载原始数据，prefetch有个最大的好处是只要知道SRA数据库的数据类型编号，就可以直接下载对应的原始数据。如果要批量下载，可以将数据编号写入txt文件中再运行prefetch命令，或者直接写个循环语句。所以这里的关键是怎么得到数据编号，比如SRR编号等等。 前面GEO数据库提供了SRA数据库的链接，我们可以直接点开（或者点击原始数据底下的SRA Run Selector）： 点击右上角的Send results to Run selector: 我们可以看到，这个项目一共有16个基因组测序数据，难道要16个wget命令一个一个下载麽？不需要，如果要下载的基因组数据数量多肯定不行。我们可以从Accession list里获取不同前缀的各种run数据，后面用prefetch命令结合循环语句，直接一步下载。 把Accession list的编号全部复制下来，以linux为例运行cat &gt; id，回车后粘贴编号，按ctrl+c退出，这样就生成了一个名为id的文件，里面内容是我们要下载的基因组测序数据编号。 写一个循环语句cat id | while read id ;do prefetch $id &amp;;done ，就可以全部下载了，这里没有指定输出目录，最终所有原始测序数据会输出到根目录下。 最后顺便说一句，下载测序原始数据的方法很多，不仅仅是官方给的这个小工具。还可以用aspera遵循一定的下载格式也可以下载原始数据，或者用最原始的wget简单粗暴直接下载，只是说这些方法都或多或少受到网络限制的影响，prefetch也只是相对稳定一点。前面的笔记我也介绍过爬虫的编程方法，分解网页结构，批量抓取我们需要的信息，这也不失为一种方法。人是活的，不要拘泥于一种思路。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"SRA","slug":"SRA","permalink":"http://www.shelven.com/tags/SRA/"},{"name":"SRA Toolkit","slug":"SRA-Toolkit","permalink":"http://www.shelven.com/tags/SRA-Toolkit/"},{"name":"GEO","slug":"GEO","permalink":"http://www.shelven.com/tags/GEO/"}]},{"title":"简易爬虫程序编程记录——以微博热搜为例","slug":"简易爬虫程序编程记录——以微博热搜为例","date":"2022-05-02T16:05:41.000Z","updated":"2022-05-02T16:07:59.402Z","comments":true,"path":"2022/05/03/a.html","link":"","permalink":"http://www.shelven.com/2022/05/03/a.html","excerpt":"","text":"1. 关于爬虫百度百科对于爬虫的定义是，网络爬虫（又被称为网页蜘蛛、网络机器人，在 FOAF 社区中，经常被称为网页追逐者），是一种按照一定的规则，自动抓取互联网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。 我们只需要知道爬虫的作用是抓取网页的信息，其实现在互联网上充斥着大量的爬虫，包括不局限于火车票抢票软件，各种实时数据分析网站等等，本质上都是发起大量的http请求获得信息。爬虫技术的滥用会导致目标网站在短时间内收到大量的访问请求，进而导致服务器瘫痪，相当于是ddos攻击了。但是爬虫的便利性是不可否认的，尤其是批量操作数据和获取信息，比如批量下载我们需要的文献等等。犯罪的永远是凶手而不是工具，我们在合法的范围内应用好工具，能为我们生活提供非常大的便利。 知其然知其所以然，了解这个技术的最好方法是自己去学，因此写了这个小爬虫程序。为什么拿微博热搜来练手呢，因为微博热搜网页结构非常简单明了，很容易上手…… 写的这个小爬虫程序主要是应用requests库和lxml包的etree库，简单介绍一下。 1.1 requestsrequests是最常用的Python HTTP客户端库，编写爬虫和测试服务器响应数据时经常会用到，专门用于发送HTTP请求。说白了requests最大的作用就是发起http请求，返回我们需要的网页数据，所谓爬虫就是从网页上抓取和整理我们需要的公开的信息，对于非公开的信息抓取是违法的。 requests请求方式： 1234567requests.get(url, kwargs)： 发送GET请求requests.post(url, kwargs)： 发送POST请求requests.put(url, kwargs)： 发送PUT请求requests.delete(url, kwargs)： 发送DELETE请求requests.head(url, kwargs)： 发送head请求erquests.options(url, kwargs)： 发送options请求这些请求方法的参数和用法一致，必选参数为url，其他参数为可选参数 1.2 etreelxml的etree是从上面requests返回的html源码中提取信息用的，我们可以通过xpath解析网页的dom树，从中获取我们需要的元素和内容。 主要用的也就是etree.HTML()，可以用来解析字符串格式的html文档对象，更方便对我们需要的元素和对象进行抓取，后面演示会说到。 2. 代码和结果展示123456789101112131415161718192021import requestsfrom lxml import etreeimport timeurl = &#x27;https://s.weibo.com/top/summary/&#x27;header = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27;,&#x27;cookie&#x27;: &quot;UOR=www.baidu.com,s.weibo.com,www.baidu.com; SINAGLOBAL=2417808258422.6777.1651037395174; _s_tentry=-; Apache=9947874618898.105.1651493077297; ULV=1651493077318:2:1:1:9947874618898.105.1651493077297:1651037395190; PC_TOKEN=04cd3c070b; login_sid_t=80fe8e3820060c4330191a42b71357dd; cross_origin_proto=SSL; ALF=1683032497; SSOLoginState=1651496497; SUB=_2A25Pa6ZiDeRhGeFL6lAT-CzMyj-IHXVsAJCqrDV8PUNbmtB-LUjdkW9NQm3k0nVgyW6LFmyhR5luy-dtvVNK1VjC; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWTcwdUO8qo5ZVwP9-2e8C.5JpX5KzhUgL.FoMfeKzE1hz7eKe2dJLoIp7LxKML1KBLBKnLxKqL1hnLBoMNSK2EeonEeh20&quot;&#125;resp = requests.get(url, headers=header)resp1 = resp.content.decode(encoding=&#x27;utf-8&#x27;)resp2 = etree.HTML(resp1)title = resp2.xpath(&#x27;//*[@id=&quot;pl_top_realtimehot&quot;]/table/tbody/tr/td/a/text()&#x27;)clout = resp2.xpath(&#x27;//*[@id=&quot;pl_top_realtimehot&quot;]/table/tbody/tr/td/span/text()&#x27;)addresses = resp2.xpath(&#x27;//*[@id=&quot;pl_top_realtimehot&quot;]/table/tbody/tr/td/a/@href&#x27;)print(time.strftime(&quot;%F,%R&quot;)+&#x27;\\n50条实时微博热搜\\n&#x27;+&#x27;\\n排列方式：序号+关键词+热度\\n&#x27;)for i in range(51): if i == 0: print(&#x27;&#x27;.join(&#x27;置顶&#x27;+&#x27;\\t&#x27;+title[i]+&#x27;\\n&#x27;+&#x27;https://s.weibo.com&#x27;+addresses[i]), &#x27;\\n&#x27;) else: print(&#x27;&#x27;.join(str(i)+&#x27;\\t&#x27;+title[i]+&#x27;\\t&#x27;+clout[i-1]+&#x27;\\n&#x27;+&#x27;https://s.weibo.com&#x27;+addresses[i]), &#x27;\\n&#x27;) 未对代码进行封装，源代码就这么十几行，实现的结果是，执行一次就在当前终端屏幕上输出实时的50条微博热搜话题，并显示序号和热度，每条热搜话题下一行生成微博超链接。 3. 代码详解建立python脚本，导入模块这步不解释了。 url是我们要抓取信息的网站地址，这个很好理解。header是我们调用requests模块需要的一个重要参数，里面提供了我们访问需要的认证信息cookie，http请求本身是无状态的，网站无法确认前一次发出请求的人和后一次发出请求的人是否为同一人，因此需要让网站记住我们的登录信息cookie以响应我们的请求。没有header可能无法返回网页信息，那这一大堆东西是怎么来的呢？需要我们审查网页元素。 3.1 获得cookie和user-agent打开微博热搜首页，登录微博，随便什么空白的地方右键，点击检查，找到network（网络）。 上面的为网页元素，日志控制台，网络，资源，性能和内存等等标签，下面的就是对应的内容，往往点击第一个总结类的文件可以获得request headers信息，这里面最重要的两个信息：cookie和User-Agent 将cookie和user-agent内容全部写到header变量中，这样每次访问网站就带上了我们唯一的标志信息 resp = requests.get(url, headers=header) 访问目标网址，返回的html源码赋值给resp，然而我们看不到返回的值是怎么样的，什么类型的，这里我就要介绍一下vscode的AREPL插件了。 3.2 AREPL查看变量和审查网站元素前面介绍vscode插件说过，AREPL可以实时打印出当前的变量信息而不需要运行代码，极大地方便了我们查看返回的值和信息，知道每一行代码发挥了什么作用。 我们看一下自定义的resp变量是什么： status_code值为200，很明显成功返回了html源码信息，但是点开来看却得不到我们需要的网页文字信息，因为还没有进行解码。我们可以看到编码方式是UTF-8，自然而然的，我们就要对resp变量值进行对应的UTF-8解码，也就是后面的代码resp1 = resp.content.decode(encoding=&#39;utf-8&#39;)，这里注意一点要用content不能用text 再来点开看看解码后的resp1： 如果有点html基础的话会发现，怎么样，是不是很熟悉！没错！这就是我们在审查网页元素获得的网页的前端结构，这里包括了所有的网页信息，再也不用点开原网站一个一个元素去找啦！（就比如我这小破站的网页元素看地我脑瓜子嗡嗡的）这里可以很轻易地看到各个节点信息，极大方便了我写上面的爬虫代码。 这里我需要三个信息，热搜的标题、热度和网址，我们来展开看一看网页结构： 像洋葱一样一层一层拨开网页结构，我们可以清楚地看到table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a节点的内容是微博热搜标题，a这个节点的标签href就是网址，table&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;span节点的内容就是热度，至此，网页结构一清二楚，我们要做的就是把信息提取出来，提取的方式就是etree解析这个字符串格式的html文档，生成对应的元素路径。 3.3 解析html文档resp2 = etree.HTML(resp1)就是用来解析字符串格式的HTML文档对象的，将传进去的字符串转变成元素对象 转换后的resp2如下，我们可以看到每个节点都被转换成了_Element对象： 接下来就是顺理成章地用xpath寻找元素路径，将对应内容提取出来，我的元素路径中应用了正则表达式，这里也不解释了。 最后可以将提取出来的三个信息一一打印出来看看是否有问题（AREPL插件真的立大功），有了信息接下来就是整理和排版，那就是print函数和循环语句的基本用法了。虽然python中的print函数和循环语句与R或者linux中略有不同，这个基础知识这里也不再赘述。 唯一我觉得需要注意的是，range() 函数提供的是0-51的整数；官网置顶的微博没有热度显示，所以我写了一个if判断语句区别；排版的时候注意转义字符，其他都是细节微调部分，怎么美观怎么顺眼怎么来。 4. 总结因为这个网页没有做反爬（或许是我没注意到）手段，至少我获取这些公开信息还是没有遇到阻碍的，这也是最最简单的一个爬虫脚本了，调用第三方模块，解析网页，最后提取信息和整理，就是这么简单也很好理解。 我最近还接触到一个明日方舟抽卡记录汇总的小程序，我看了下小程序的方法，猜测这类程序也是类似的爬虫程序。先登录官方网站，需要你输入一个网址，提供token_by_cookie这个值，这个值在network标签中能找到，并且能发现resquest url就是获取token_by_cookie值的网站。 获得这个token之后，我们可以看到抽卡记录可以通过另一个需要token的网址中直接获取，明日方舟的抽卡记录只能保存10页，因此，只需要输入token，直接更改page值1-10，就能获得详尽的抽卡信息。而做成好看的图表也无非是把爬虫程序和作图程序结合一下，封装，最后在小程序调用，思路就是这样的。 同时也能发现，如果我们提供token值给别人，除了抽卡记录以外，还能提取我们的充值信息和源石消费信息等等这类隐私信息。要掌握隐私信息无非是程序的作者想不想做的问题，毕竟还是把隐私信息握在自己手里比较好。 学习就是学习这些技术的思路并为自己所用。","categories":[{"name":"编程自学","slug":"编程自学","permalink":"http://www.shelven.com/categories/%E7%BC%96%E7%A8%8B%E8%87%AA%E5%AD%A6/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.shelven.com/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.shelven.com/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"vscode远程连接和快速搭建python环境","slug":"vscode快速搭建python环境","date":"2022-04-29T10:17:01.000Z","updated":"2022-04-29T16:04:10.698Z","comments":true,"path":"2022/04/29/a.html","link":"","permalink":"http://www.shelven.com/2022/04/29/a.html","excerpt":"","text":"最近在自学python，刚入门苦于不知道从何下手，也不知道用什么编辑器比较适合。在度娘上搜了十几款编辑器，最终决定用微软的vscode，这个编辑器可以配置Python、Java、C ++等编程环境，而且有非常强大的插件功能，界面看着也挺友好，写个日志记录下自己瞎捣鼓的配置。 本来是想在我的云服务器上装vscode，但是我的云服务器上没有可视化界面……于是在我的小破笔记本上安装了vscode，后来又发现有一个插件可以ssh连接上服务器，只要能ssh连接就可以直接调用服务器上事先安装好的各种python库，真香~ 从头开始记录下使用方法和自己的设置 1. 下载vscodevscode可以直接上官网下载（速度很慢，建议科学上网），选择自己的操作系统，我用的windows 一直下一步就可以了，唯一需要注意的是把vscode加入系统环境变量中（默认选项），安装以后能够在cmd命令行通过code打开说明就改了环境变量。当然也可以在系统环境变量的path中找到，这里不赘述 2. 插件下载2.1 中文语言包英文界面对于我这种小白太难了，所以打开软件第一件事就是安装中文插件，这个在左边拓展栏输入chinese直接可以找到（我这里已经装好了，只是演示记录一下） 2.2 ssh连接插件因为我要远程调用服务器上的python库，所以我首先下载了SSH连接插件 安装以后点击右侧菜单栏的远程资源管理器，可以新建一个远程连接 按照正上方弹出的窗口提示，我们输入自己的用户账号和host地址，之后选择第一个选项，这样我们要连接的远程主机地址就被记录下来了。连接之后输入密码即可远程登录，每次登录都需要输入密码 连接以后可以选择打开文件夹，把根目录文件夹打开就可以调用远程服务器的所有文件了 看到终端成功显示欢迎界面，说明远程登陆成功，终端可以输入和执行命令了 2.3 python拓展插件远程登录只是第一步，接下来安装插件都是远程登录的窗口，安装在本地的插件一般不能用在远程登录窗口。我要搭建python环境，也是先安装python的拓展插件 2.4 AREPL插件这个插件可以在右上角点开，实时打印出你写的python脚本运行结果，变量的赋值等等，还可以检查你写的脚本哪里出错而不需要直接运行代码。这个插件在写爬虫脚本的时候真的非常方便（后面的笔记再分享，写个简单的爬虫脚本就能体会），再也不需要点开网页审查各个元素了，直接在右边框里找到 举个栗子比如我写了个打印皮卡丘的python脚本（滑稽），我不用运行程序就能在右边看到代码运行的结果 3. 脚本调试下载完插件，写完代码，我们首先要进行的就是代码调试，点击左侧菜单栏的 运行和调试 ，我们直接点击创建 launch.json文件 在弹出的正上方菜单栏选择第一个调试配置打开的python文件 本质上就是生成一个调试的json文件，不用太多了解，调试当前打开的文件就可以。也可以根据自己需要改成只调试指定名称的python脚本，改的就是红框里的部分 在脚本页面直接按F5就可以运行了，可以在代码的行号前设置红色的断点，用来分段测试代码，在写的代码比较多需要一段段检查错误的时候会比较有用。比如我在上面的皮卡丘脚本的第12行设置一个断点，再按F5运行脚本，就会从第一行报错（因为print函数被断点隔开了），右边是AREPL插件的输出结果，不受代码运行与否和断点的影响，所以能正常显示 我们同时也能看到右下角是bug调试区，也就是说我们在调试区运行程序，会自动给我们分配一个debug控制区的终端，我也可以同时在上面的bash区终端运行别的程序。 4. 格式化文档这几天自学过程中我也发现，python代码是有严格的缩进要求的，不像是linux系统中的shell语言，一行写完可以在另一行随便插几个制表符继续写下一个命令。python严格按照冒号和缩进来区分代码块之间的层次，在 Python 中，对于类定义、函数定义、流程控制语句、异常处理语句等，行尾的冒号和下一行的缩进，表示下一个代码块的开始，而缩进的结束则表示此代码块的结束。哪怕有一个多余的缩进量，就会系统报错 在赋值前后，运算符前后，#号注释之后等一些不用区分代码块层次的地方，python对空格要求却不是那么严格。虽然要求不严格，但是不小心手滑多打了或者少打了空格总归影响美观（我真的有强迫症），这个时候可以用右键的格式化文档功能，一键自动改成标准格式 比如上面这个丑不拉几的程序虽然能跑出来结果，但是强迫症看完会当场去世。这个时候可以用右键的格式化文档功能一键对齐，如下 如果点格式化文档显示的是要装autopep8拓展，那就点确认安装。这个功能就是靠autopep8这个软件实现的，但是这个拓展软件是靠pip安装的，有的人没有安装pip，或者有的人（比如我）pip有问题，一直显示ssl证书不能获取拒绝安装，改pip源也无法解决问题（又是套娃解决问题的一天），就要去github下载autopep8本地安装，更改环境变量才可以使用。也可以用conda直接一步安装，不得不说conda管理python环境变量是真的香 格式化文档也不是万能的，比方说我在父目录下封装了一个函数，我想在子目录下调用，vscode有一个缺点就是需要把当前目录加到环境变量里，然后才能调用我封装好的函数，但是！添加环境变量之后再调用，这个时候运行格式化文档的功能，系统会自动把调用模块排在添加环境变量步骤之前，因为软件的设计就是把调用模块一定放在第一位。 打个比方，我在父目录demo下封装了printpikaqiu()这个函数，这个函数作用是打印皮卡丘。文件目录如下 然后我在子目录test下调用，就需要先拓展环境变量再调用，代码如下图，f5运行没毛病，打印出一只皮卡丘： 但是右键格式化文档之后，代码直接变了，再运行直接红色的报错跳脸上 因为代码顺序改了，没有添加环境变量，找不到父目录demo怎么可能调用pikaqiu模块呢？这就非常尴尬了 因此在调用自己封装的函数和模块的时候，不要用格式化文档。现在暂时还没找到可靠的解决方法 5. 其他设置vscode主菜单栏 文件—首选项 底下有非常多非常详细的设置选项，而且可以不同设备进行同步，这个是其优点之一。并且可以通过ctrl + shift + p 快速打开设置，支持直接修改配置的json文件，这个暂时还没用到，我只是改了个字体大小，以后有重要修改的时候再做记录，方便后续查看。 顺便提一嘴，我的云服务器是安装了anaconda管理python环境的，用vscode远程ssh登录云服务器后，仍然可以在右下角选择我用anaconda安装的各个版本python，依然是一键切换环境，真的太方便了。之前没用编辑器码代码，每次都要在linux里通过vim码好保存退出，再运行。。。各种意义上的身心折磨。。。 现在就一个字，香！","categories":[{"name":"编程自学","slug":"编程自学","permalink":"http://www.shelven.com/categories/%E7%BC%96%E7%A8%8B%E8%87%AA%E5%AD%A6/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://www.shelven.com/tags/vscode/"},{"name":"python","slug":"python","permalink":"http://www.shelven.com/tags/python/"}]},{"title":"转录组数据分析笔记（8）——ggplot2和ggrepel绘制火山图","slug":"转录组数据分析笔记（8）——ggplot2和ggrepel绘制火山图","date":"2022-04-24T20:53:27.000Z","updated":"2022-04-24T20:58:08.201Z","comments":true,"path":"2022/04/25/a.html","link":"","permalink":"http://www.shelven.com/2022/04/25/a.html","excerpt":"","text":"1. 前言前面介绍了怎么用DESeq2做两组样本的差异基因表达分析，以及怎么用dplyr包给DESeq2运行结果增加一列分组信息，我们先看下两个R包运行结束后生成的gene_0_1.csv文件是怎么样的： A-G列结果是DESeq2跑的，我们用到的只有基因名，log2FoldChange和padj这三列，通过log2FoldChange绝对值大于1，调整后的pvalue也就是padj（即FDR值）小于0.05筛选除差异表达基因，最后加上group列方便查看。 到这里已经可以通过排序找到我们感兴趣的基因了，但是这样的数据不够直观，我们还可以用最著名的绘图R包ggplot2做个火山图。这里需要准备的绘图R包是ggplot2，还有添加标签的R包ggrepel。 1.2 两个注意点什么是火山图就不多bb了，重要的是知道我们可以从火山图获得两个信息：差异表达倍数（FoldChange值）和统计学显著性的标志p值。为了更方便比较和作图，我们一般用log2FC代替Fold Change值并作为X轴数据，表示两样品（组）间表达量的比值，对其取以2为底的对数即为log2FC，一般默认取log2FC绝对值大于1为差异基因的筛选标准；用FDR（也就是padj值）代替pvalue，并取-log10（FDR）值作为Y轴，FDR是错误发现率，是pvalue值进行校正得到的。 log2FC有正有负很好理解，可能有同学发现，有的基因明明有pvalue值，但是校正之后的FDR值却是NA，如下： 查阅了一些资料，当基因的在所有样本中表达量为0，则两个值都为NA；当read count数较低时，DESeq2进行Independent Filtering过滤了一部分可能造成假阳性的结果，此时padj值为NA。因此，这部分数据在做火山图的时候因为没有对应的Y值也会被过滤掉。 2. 流程代码这部分需要一点R语言基础，需要知道怎么改自己的参数。假设前面没有用dplyr包做差异筛选（做了也不影响，只是多一列数据而已）只用DESeq2跑了个结果，我们同样可以用ggplot2包做筛选，用ggrepel包美化做标签。继续用前面DESeq2生成的csv文件，流程代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748library(&quot;ggplot2&quot;)gene &lt;- read.csv(&quot;gene_0_1.csv&quot;, stringsAsFactors = FALSE)gene[which(gene$log2FoldChange &lt;= -1 &amp; gene$padj &lt; 0.05), &quot;GROUP&quot;] &lt;- &quot;DOWN&quot;gene[which(gene$log2FoldChange &gt;= 1 &amp; gene$padj &lt; 0.05), &quot;GROUP&quot;] &lt;- &quot;UP&quot;gene[which(abs(gene$log2FoldChange) &lt; 1 | gene$padj &gt;= 0.05), &quot;GROUP&quot;] &lt;- &quot;NOT CHANGE&quot; # |代表或，和linux里的管道是完全不一样的。以上三步新建了一列GROUP，筛选并赋予了三个值。res &lt;- ggplot(gene, # ggplot数据来源，这里省略了data = 和mapping = aes(x = log2FoldChange, # 表示映射关系，就是定义xy y = -log10(padj), col = GROUP)) + # 注意这里定义颜色用col，以GROUP值区分 geom_point(alpha = 0.5, # ggplot做散点图，设置点透明度和大小 size = 1) + scale_color_manual(values = c(&quot;red&quot;,&quot;blue&quot;,&quot;grey&quot;), limits = c(&quot;UP&quot;,&quot;DOWN&quot;,&quot;NOT CHANGE&quot;)) + # 自定义颜色 theme(panel.grid = element_blank(), # 去网格线 panel.background = element_rect(color = &quot;black&quot;, fill = &quot;transparent&quot;), # 去背景色，透明 plot.title = element_text(hjust = 0.5), # 调整图标标题位置为中间 legend.key = element_rect(fill = &quot;transparent&quot;), legend.background = element_rect(fill = &quot;transparent&quot;), legend.position = &quot;right&quot;) + # 设置legend图标 geom_vline(xintercept = c(-1, 1), color = &quot;gray&quot;, size = 0.3) + # 设置x轴辅助线 geom_hline(yintercept = -log10(0.05), color = &quot;gray&quot;, size = 0.3) + # 设置y轴辅助线 labs(x = &quot;log2 Fold Change&quot;, y = &quot;-log10(FDR) &quot;, title = &quot;LD 1 day vs LD 0 day&quot;) # 设置坐标轴标题和火山图标题res # 查看结果，plots中可以查看library(&quot;ggrepel&quot;)up &lt;- subset(gene, GROUP == &quot;UP&quot;) # subset从数据框中筛选符合条件的数据up &lt;- up[order(up$padj), ][1:5, ] # order升序排序，取前5个down &lt;- subset(gene, GROUP == &quot;DOWN&quot;)down &lt;- down[order(down$padj), ][1:5, ]resdata &lt;- res + geom_text_repel(data = rbind(up, down), # ggrepel特有的函数 aes(x = log2FoldChange, y = -log10(padj), label = X ), # label值指定哪列做标签 size = 3, box.padding = unit(0.5, &quot;lines&quot;), segment.color = &quot;#cccccc&quot;, show.legend = FALSE) # 以上都是特有参数resdata # 查看结果ggsave(&quot;gene_0_1.png&quot;, resdata, width = 10, height = 10) # 输出结果文件 3. 代码详解3.1 ggplot2123gene[which(gene$log2FoldChange &lt;= -1 &amp; gene$padj &lt; 0.05), &quot;GROUP&quot;] &lt;- &quot;DOWN&quot;gene[which(gene$log2FoldChange &gt;= 1 &amp; gene$padj &lt; 0.05), &quot;GROUP&quot;] &lt;- &quot;UP&quot;gene[which(abs(gene$log2FoldChange) &lt; 1 | gene$padj &gt;= 0.05), &quot;GROUP&quot;] &lt;- &quot;NOT CHANGE&quot; 之前这里稍微解释一下，即使前面没有用dplyr包，用别的方法同样可以筛选差异基因并且新增一列分组数据，万变不离其宗，核心的判断方式是一样的。如果前面学了linux，注意最后 | 这个符号在R里表示或，不要和管道混淆。 123456ggplot(data = 输入的数据, mapping = aes(x = 定义值, y = 定义值, 其他参数)) + genom_point(参数) + # 选择作图方法和参数 其他设置函数和参数 # 可有可无，美观相关的东西 我总结了一下ggplot最基本的结构，data和mapping是缺省值，可以不写。 输入的数据可以是表格，可以是数据框等等；aes自定义点的映射范围，大小，颜色等等；作图方法有很多，比如点状图是genom_point。自由度很高，能设置的东西也非常之多，只有两点需要注意，同一个函数不同参数用 , 隔开；不同函数用 + 隔开。 中间的设置函数也稍微解释一下： scale_color_manual() 该函数是R中的一种自定义配色方法，手动把颜色赋值给参数value。我这里将UP的点赋予了红色，DOWN的点赋予蓝色，其他点赋予灰色。 theme() 该函数与主题配置有关，参数非常多，可以选取需要的比如背景色、网格线等等进行设置。这里举个例子，legend是图标，在ggplot中legend有四部分： legend.tittle, legend.text, legend.key和legend.backgroud，而每一个部分都有四种函数可以嵌套（也是是对应4种处理方式）：element_text()绘制标题相关；element_rect()绘制背景相关；element_blank()空主题，对元素不分配绘图空间；element_get()得到当前主题的设置。每个函数还有相应的参数，说起来就没完没了了。。。常用的设置知道就行。 geom_vline() 和 geom_hline() 这两个函数分别设置x轴y轴辅助线，目的是使我的火山图更直观，从图上可以直接看到我的分类依据。 labs() 该函数自定义x轴y轴标签和图标标题。这里提一嘴，火山图标题也是一个注意点，一般是 处理组vs对照组 ，因此前面也说到DESeq2处理数据要注意顺序问题，不然会得出完全相反的结论，在火山图上的表现为与实际火山图呈镜像对称，这也很好理解。 这里看一下res结果，我们可以在plots中看到缩略的预览图： 3.3 ggrepel在过滤掉4000多个FDR值为NA的点，我们获得了一个不怎么像火山的火山图 （简直丑爆了） ，但是数据还是挺美丽的，上调区域和下调区域都有前后对比差异非常大的基因：log2FC绝对值越大，差异越明显；-log10（FDR）值越大，可信度越高。 但是这个图还有个缺点，我不知道这些代表性的差异点对应什么基因名，我还要回到excel里去筛选排序。因此，我推荐用ggrepel包对火山图进一步美化，加上基因标签，能一眼看到我感兴趣的基因。 这个包的原理和发展咱就不说了，已经是半夜4点了。。。简单介绍下中间用到的函数的结构。 subset() 从数据框中筛选符合条件的数据，我将UP的点和DOWN的点都提取出来。 order() order是升序排序，因为上调和下调的基因都比较多，全部打上基因名标签是不现实也没有意义的。我按照padj列也就是FDR值进行升序排序，取前5个可信度最高的基因打上基因名标签。当基因较少的时候是可以全部打上标签的。 在前面ggplot2作图的基础上，我们加上geom_text_repel()这个特殊的ggreple包函数，这个函数是基于函数geom_label()做的改良，它将标签置于一个方框中，并且每个标签有算法优化不会重叠。该函数的结构与前面的ggplot前半部分类似： 12345geom_text_repel(data = 输入数据, aes(x = 定义值, y = 定义值, label = X ), 其他参数 这里所有参数设置都是平行的，所以只需要 , 隔开。 我之前说到提取了up和down的数据，这里我把它们rbind一下合在一起，就形成了新的数据框数据，也就是我只对前面排序筛选的上调下调各5个基因打标签。这里注意下aes()这里的 label 是指定标签的，也就是我们这里的基因名，应该用的行名才能和数据一一对应，这里我用X是偶然发现的一个很有趣的事： 前面导入gene数据框的时候，自动把行名加到了第一列成了单独的一列，且该列列名系统定义为X，我们可以进入environment找到gene点开看看这个数据框结构，如下所示： 因此这里直接用label = X就能完美解决问题。反而我回头用row.names = 1用第一列做行名修改了gene数据框的读取方式，再在这里用label = rowname(gene)会提示长度错误或者不匹配，个中原因我暂时还没想明白。 其他特有参数就解释一下我用到的几个： size: 标签大小 box.padding: 标签连接方式，我用了线 segment.color: 线段颜色，可以用RGB颜色代码 show.legend: 是否显示标签的标签 &#x3D;_&#x3D;好像有点绕，说白了就是要不要再图标上再打标签… 同样放一张plots里的缩略图，是不是要直观一点了呢？ 3.4 结果输出ggsave()是ggplot2包里的输出结果的函数，自定义输出的文件类型，比如pdf、png等等，还可以自定义输出图片大小，这里不赘述，主要放一个完成图看看和plots里的缩略图做个比较。完成图如下： 可能还是有些不美观，但是这些数据很不错，极端点偏离X轴和Y轴较远，都是我们需要重点关注的基因。 如果我们记下了这几个极端点，我们还可以通过在ggplot中限制X轴和Y轴范围比如xlim(-10, 10) + ylim(0, 14)，再次缩小范围，得到一个更像火山的火山图 (没有意义，纯粹吃饱了撑的) 如下：","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://www.shelven.com/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"ggplot2","slug":"ggplot2","permalink":"http://www.shelven.com/tags/ggplot2/"},{"name":"ggrepel","slug":"ggrepel","permalink":"http://www.shelven.com/tags/ggrepel/"}]},{"title":"linux操作指令总结整理","slug":"linux操作指令总结整理","date":"2022-04-20T15:44:49.000Z","updated":"2022-04-26T05:42:35.768Z","comments":true,"path":"2022/04/20/b.html","link":"","permalink":"http://www.shelven.com/2022/04/20/b.html","excerpt":"","text":"该篇内容非常之多，主要记录自己能用的上的linux操作指令和自己的一些理解，想要用的时候方便站内搜索直接查找 1. linux常用命令cdcd：Change directory修改（进入）工作目录，只对目录文件有效 12345cd / 进入根(root)目录cd - 返回上次的目录cd 返回家(home)目录cd ~ 返回家目录cd .. 返回上一级目录 ls ls：List filesls计算不了目录内文件大小，所以显示的目录大小不是实际的，要看目录实际大小用du命令 123456-a 列出包括.a开头的隐藏文件的所有文件-A 通-a，但不列出&quot;.&quot;和&quot;..&quot;-l 列出文件的详细信息-c 根据ctime排序显示-t 根据文件修改时间排序-h 将文件大小按照易于读懂的方式显示（多少M，多少G） ll和ls-l是同样的用法，linux可用，mac中不能用，可以改环境变量文件自定义ll用法 pwdpwd：print working directory，打印当前所在目录 cpcp: Copy file拷贝并粘贴文件，并且可以重命名 12345-b 覆盖前做备份-f 如存在不询问而强制覆盖-i 如存在则询问是否覆盖-u 较新才覆盖-t 将多个源文件移动到统一目录下，目录参数在前，文件参数在后 $ cp ../data/xist.fa xist_seq.fa # 复制上一个目录data目录下的xist.fa到当前目录，并重命名为xist_seq.fa$ cp -r 003/ 007 # 递归的方式，复制003目录到007目录，目录复制到目录要用递归 mvmv: Move file移动文件，相当于windows下的剪切粘贴，如果剪切粘贴到同一目录下，则为重命名 12345-b 覆盖前做备份-f 如存在不询问而强制覆盖-i 如存在则询问是否覆盖-u 较新才覆盖-t 将多个源文件移动到统一目录下，目录参数在前，文件参数在后 $ mv a1.index.sh ../ # 移动到上一目录$ mv a1.index.sh a2.index.sh # 重命名为a2.index.sh$ rename txt doc * # 把所有txt改成doc，批量文件重命名可以用rename rmrm: Remove file删除文件 1234-r 删除文件夹（就是删除目录）-f 删除不提示-i 删除提示-v 详细显示进行步骤 一定要慎重使用，命令行模式下删除文件不可恢复$ rm -rf *.fna #删除目录下所有以.fna结尾的文件 lnln: Link files创建连接文件，包括软连接和硬链接，一般软连接比较常用，相当于windows下的快捷方式；硬链接相当于重要文件的备份，默认硬链接删除原文件，硬链接文件不受影响，软连接文件则无效 12-s 建立软连接 -v 显示详细的处理过程 mkdirmkdir：Make directory创建文件夹(目录) 123-p 递归创建目录，若父目录不存在则依次创建-m 自定义创建目录的权限-v 显示创建目录的详细信息 $ mkdir rnaseq #创建一个名为rnaseq的目录 touch建新的空文件(可写入的文件)$ touch 1.txt 2.txt 3.txt # 同时新建三个文件，一个文件可以直接vim建立 catcat: concatenate 连接cat的一个作用是查看文件，一般是比较小的文件，行数小于一个屏幕，最多不要超过两个屏幕，否则会刷屏（屏幕输出的方式）cat另一个作用是合并多个文件，一般配合重定向合并为一个新文件或者将一个文件内容追加到另一个文件结尾$ cat a1.txt a2.txt &gt;all.txt # 合并文件，并不会删除原文件，覆盖新文件内容，新文件为all.txt$ cat a1.txt &gt;&gt;a2.txt # 同样是合并，a1重定向到a2结尾$ cat &gt;id.txt # 回车输入内容，可新建id.txt文件，ctrl+c退出 echo不可以这样新建，只能echo &quot;内容&quot;&gt;id.txt 1-A 显示文件内的空白信息 linux系统下是换行\\n；mac系统下是回车\\r；windows系统下回车加换行两个字符\\r\\n 三者都是空白，用less无法看出区别，只能在cat -A下看到不同操作系统的换行符信息 less &#x2F; moreless和more都是文件查看工具，但是less功能更多一些，在windows系统下打开一个10G的文件比较困难，但是在Linux下非常方便，less可以打开非常大的文件，压缩格式也可以直接打开。注意后面接文件，不能接目录。 12-m 显示类似于more命令的百分比-N 显示行号 more：q退出，space向下翻一页，enter向下滚动一行，b往前翻一页，会加载全部显示浏览到百分之几，退出后会加载显示的所有内容less：类似，还可以用pageup和pagedown，不会加载全部，退出后不会加载文件内容显示到当前界面less下按h进入帮助界面；按&#x2F;向下搜索字符串，按？向上搜索字符串，搜索状态下n和p前后跳转；按v进入编辑 head &#x2F; tail这两个命令比较简单，只是取一个文件的头部和尾部多少行，默认10行，可以加-n进行设置，利用管道可以取文件中间行$ head -40 a.txt | tail -20 #取文件第21~40行$ tail -n +20 notes.log #取文件的第20行到文件末尾 g(un)zip&#x2F; b(un)zip2gzip和bzip2是文件压缩工具，默认直接对源文件进行处理，压缩比率在2&#x2F;3左右，都可以进行设置加上un，为unpack的意思，表示解压缩linux压缩文件格式是.gz和.bz2windows压缩文件有.rar文件，可以下载rarlinux工具解压缩；.zip文件可以通过unzip命令解压bzip2压缩比更高（尽量下载bz2压缩文件），但是占用更多CPU$ gzip a.txt # 压缩a.txt文件$ gunzip a.txt.gz # 解压a.txt.gz文件压缩的文件可以用less或者zcat打开文件 tar（很多生物软件是打包并压缩的）tar：Tape archive （磁带档案）tar主要用于打包，由于tar能调用gzip或者bzip2进行压缩，而打包和压缩经常如windows系统一样合并为一个过程 123-c 建立打包档案，可搭配 -v 来察看过程中被打包的档名(filename)-t 察看打包档案的内容含有哪些档名，重点在察看文档名就是了（同less功能）-x 解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 以上三个命令不能同时使用，只能三选一辅选项： 1234-j 透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2-z 透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz-v 在压缩/解压缩的过程中，将正在处理的文件名显示出来！-f filename -f 后面要立刻接要被处理的档名！f很重要，每次执行tar命令都要加上 对于初学者，记住c是creat，创建，x是解包，z对应gzip，j对应bzip2即可，所以常用的命令如下：$ tar -jcvf filename.tar.bz2 A B C #打包压缩为bz2结尾文件$ tar -jxvf filename.tar.bz2 # 解压缩.tar.bz2结尾文件$ tar -zcvf filename.tar.gz A B C #打包压缩为gz结尾文件$ tar -zxvf filename.tar.gz # 解压缩.tar.gz 结尾文件$ tar -jxvf filename.tar.bz2 -C 目录名 #解压缩到指定目录，注意是大写Cless命令可以不解压只查看（真的强大），tar -tf filename同样如果只需解压其中一个文档，可以先通过-t查看文档名并复制，再在前面解压缩的命令基础上加空格和文档名 wcwc &#x3D; Word Count统计一个文件中，行数，单词数（有空格或者换行符的字符串），字符数 1234-l filename 报告行数-c filename 报告字节数-m filename 报告字符数-w filename 报告单词数 统计当前目录下有多少文件$ ll | wc # 注意显示行数比实际多两行，因为还有隐藏的当前目录.和上一层目录.. 可通过ls -a查看 sort排序，默认按第一列排序，可以通过-k进行设置；默认排序规则为按ASCII码排序，可以通过-n进行修改；-r取相反方向； 12345-n 依照数值的大小排序。-o 将排序后的结果存入指定的文件。-r 以相反的顺序来排序。-t 指定排序时所用的栏位分隔字符。-k 选择以哪个区间进行排序。 $ sort -nk2 -k1 01.txt | less # 在01.txt文件中，根据第二列数字大小进行排序，数字一样的比较第一列并排序 uniq用于检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用，排序之后使用uniq 1234-u 显示未重复的行-c 统计重复行的数量（在行首标注）-ci 忽略大小写统计重复行-d 显示重复出现的行 # cut -f 1 blast.out | sort -t &quot;|&quot; -nk2 | uniq | wc -l #从blast.out文件中提取第一列（f代表字段），第一列字段以“|”分割并比较第二段的数字大小进行排序，去除重复行，并记录行数 即记录有多少条比对上的基因 dfdf: disk freedf用于查看磁盘消耗，显示磁盘可用空间数目信息及空间结点信息。一般加一个-h选项，然后接要查看的磁盘，默认所有磁盘。 12345-a 显示全部文件系统-h 文件大小友好显示-l 只显示本地文件系统-i 显示inode信息-T 显示文件系统类型 dudu: Disk usagedf用于查看磁盘使用情况，du用于查看目录所占磁盘大小，一般也加-h选项 12-h 方便阅读的方式（显示带单位）-s 只显示总和的大小 findfind顾名思义，主要用于查找文件。因为当文件越来越多的时候，由于Linux是文本界面，不方便可视化文件，这个时候就可以利用find快速找到需要的文件。find支持多种搜索方式主要用的搜索方式：find 目录 Expression 条件$ find /media/ -name *.fna #查找media目录下所有.fna结尾的文件$ find /media/ -size 100M #查找media目录下所有大于100M的文件 which$ which filename # 查看可执行文件的位置，在PATH变量指定的路径中查看系统命令是否存在及其位置 whereis该指令只能用于查找二进制文件、源代码文件和man手册页，一般文件的定位需使用locate命令 locate是find -name的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库&#x2F;var&#x2F;lib&#x2F;locatedb，这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库 toptop可以动态显示（3s一次）系统进程使用情况，类似于windows系统的任务管理器。可以显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等。 psps: process statusps也是系统进程管理工具，与top不同的是，top可以动态显示，而ps则是静态显示，是某一时刻的快照，静态显示的好处是便于其他程序捕获结果，进行处理。 killkill的作用是杀死进程，给定一个任务的PID号，可以通过top或者ps命令获得，例如当前有一个sleep进程，pid号为12000；通过kill -9可以强制杀死$ kill -9 12000 12345671 终端断线2 中断，相当于ctrl+c2 退出，同ctrl+\\9 强制终止15 终止进程，默认为1518 继续，与STOP相反，fg/bg命令19 暂停，同ctrl+z chmodchmod: Change mode用于修改文件权限，Linux基础权限可以包括ugo模式（文字设定法）以及421模式（数字设定法），可以用通配符一次修改所有类型的文件文字设定法：u表示属主(user)，g表示同组群用户(group)，o表示其他用户(other)，a表示所有用户(all) 123456+ 添加权限- 删除权限= 赋予给定权限，并取消其他所有权限r 可读(read)w 可写(write)x 可执行(execute) 数字设定法： 12340表示没有权限，1表示可执行权限，2表示可写权限，4表示可读权限7：可读可写可执行 4+2+16：可读可写 4+25：可读可执行4+1 $ chmod 721 a1.index.sh # 421模式修改与之类似的还有chown与chgrp，这两个权限更大，需要root权限；chown: Change owner$ chown 用户名 目录名/ # 修改目录的属主chgrp: Change group$ chgrp 组名 目录名/ # 修改目录的组名 exit退出登录，exit是正确退出，最好不要直接点windows关闭窗口按钮退出，也不要使用ctrl+D给定退出信号退出。 man详细解释命令，系统命令可以用这个找，下载的程序往往是–help wget后面接下载网址，可以直接由地址获取下载文件 su：super user获得超级管理员权限，root权限，需要输入密码sudo：super user do暂时取得root权限，配置系统经常能看到sudo yum echo在标准输出（屏幕）上显示文字 12-n 输出之后不换行，去除结尾的换行符。注意默认一行后有一个换行符-e 转义字符按照对应方式处理 yum（centos是yum，ubuntu是apt） Yellow dog Updater Modified是一个软件包管理器，能够从指定的服务器自动下载rpm包进行安装并且自动处理依赖性关系，yum优点提供了查找、安装、删除某一个、一组甚至全部软件包的命令，并且命令简洁便于使用。 1234567891011121314151617181920yum clean all # 清除原有yum缓存yum repolist # 列出仓库信息yum install software # 安装yum update # 更新yum list software # 查看软件yum list all # 查看所有软件yum list installed # 列出已安装软件yum list available # 列出可安装软件yum reinstall software # 重新安装yum remove software # 卸载yum info software # 查看软件信息yum search software # 根据软件信息查找软件yum whatprovides file # 根据文件找出包含此文件的软件yum history # 查看系统中软件管理信息yum history info 数字 # 对该数字为id的信息进行显示yum groups list # 列出软件组 yum groups info # 查看软件组的信息yum groups install sfgroup # 安装软甲组yum groups remove sfgroup # 卸载软件组yum repolist # 查看yum源信息 cut命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段写至标准输出如果不指定 File 参数，cut 命令将读取标准输入。必须指定 -b、-c 或 -f 标志之一 1234-b 以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志-c 以字符为单位进行分割-d 自定义分隔符，默认为制表符-f 与-d一起使用，指定显示哪个区域 xargs与管道不同，xargs可以给下个命令传递参数。$ ls *.gz | head #只可以输出前10个文件名$ ls *.gz | xargs head #输出.gz结尾的所有文件前10行这里要注意下其实命令是有省略的，完整应该是ls *.gz | xargs -i head&#123;&#125; #传递参数到head的花括号中 jobs查看当前在后台执行的命令，可查看命令进程号码 &amp;运行命令时，在命令末尾加上&amp;可让命令在后台执行 顺便说一下 | ; &amp;&amp; ||区别 1234&amp;&amp; 左边命令成功运行了，右边命令才会运行，就是逻辑与的功能; 不管左边命令有没有成功运行，右边命令都会运行，两者之间独立| 左边命令的结果作为右边命令的参数，注意与xargs区分|| 左边运行的命令失败，右边的命令才会运行，否则只显示左边命令运行结果 nohup命令可以使命令永久的执行下去，和终端没有关系，退出终端也不会影响程序的运行； &amp; 是后台运行的意思，但当用户退出的时候，命令自动也跟着退出。 那么，把两个结合起来nohup 命令 &amp;这样就能使命令永久的在后台执行 fg N将命令进程号码为N的命令进程放到前台执行，同%N #注意是进程号不是PID！kill程序需要PID bg N将命令进程号码为N的命令进程放到后台执行 cal 显示日历 date 显示时间 2. 基本操作源码编译安装软件都有Readme文件或者install文件说明安装方式，一般是以下步骤：1、运行configue脚本 #检查系统环境配置情况，缺少哪些东西，缺少的可以yum下载安装2、运行make check命令（可选）3、敲make命令进行编译4、make install命令安装，出现可执行程序 文件校验下载大的文件会附带.md5文件任意长度信息逐位计算，产生128位hash值，不可逆。也就是说MD5算法可以位任何文件产生一个独一无二的数据指纹，通过校验下载前后的MD5值是否发生改变，就可以知道源文件是否被改动$ md5sum filename &gt; data.md5 # 对文件（可多个文件）生成md5校验码（32位，16进制），并命名为data.md5$ md5sum -c data.md5 # 校验文件，如果校验码相同则显示OK 重定向本质是将输出到屏幕的内容重定向到一个新的文件夹中，大于号和小于号都是代表数据的流向$ echo “想要的内容”&gt; 文件名 #覆盖原文件的内容$ echo “想要的内容”&gt;&gt; 文件名 #想要的内容追加到文件后，原文件内容不修改一个&gt;是覆盖，两个&gt;&gt;是追加 Ctrl+C终止并退出前台命令的执行，回到SHELL Ctrl+Z暂停前台命令的执行，将该进程放入后台，回到SHELL 3. vimvim（主要用来写脚本，编辑文件）vim是Linux系统自带的文本编辑器，可以理解成为windows系统下的word软件，适合编辑小文件，会一次加载全部内容到内存 123:w filename 将文件以指定的文件名保存起来 :wq 保存并退出:q! 不保存而强制退出 注意vim是vi的拓展，有些自定义设置要在vim下生效，最好是用vim用户设置优先级高于全局设置，设置文件都在家目录~下设置，且均为点开头的隐藏文件，如下~&#x2F;.vimrc~&#x2F;.bashrc 3.1 命令行模式功能键：1）插入模式 123i 切换进入插入模式 insert mode ，按&quot;i&quot;进入插入模式后是从光标当前位置开始输入文件a 进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字o 进入插入模式后，是插入新的一行，从行首开始输入文字 2）从插入模式切换为命令行模式按 ESC 键3）移动光标直接用键盘上的光标来上下左右移动，也可以用小写英文字母h、j、k、l，分别控制光标左、下、上、右移一格。 1234567G 移动到文件末尾，15G移动光标至文章的第15行行首gg 移动到文件开头$ 移动到光标所在行的行尾^ 移动到光标所在行的行首H 光标移动到这个屏幕的最上方那一行的第一个字符M 光标移动到这个屏幕的中央那一行的第一个字符L 光标移动到这个屏幕的最下方那一行的第一个字符 4）删除文字 123x 每按一次，删除光标所在位置的后面一个字符X 大写的X，每按一次，删除光标所在位置的前面一个字符dd 删除光标所在行 1,6d删除1到6行 5）回复上一次操作 1u 如果误执行一个命令，可以回到上一个操作。按多次u可以执行多次回复 6）继续下一个操作 12n或. 比如查找一个字符串以后，继续寻找下一个字符串，按多次n执行多次操作N 与 n 刚好相反，为反向进行前一个搜寻动作 3.2 底线命令模式1234567:/word # 查找word字符串:%s/x/y/gc # 所有x被y替换 g代表全局，c代表交互模式（每次替代会提示）:!命令 # 命令先执行，vim被挂起。执行后按enter回到vim:split # 横屏分屏显示 ctrl+ww切换上下屏:vsplit # 纵向分屏:only # 取消分屏:n1,n2s/word1/word2/g # 在第n1与n2行之间寻找word1这个字符串，并将该字符串取代为word2 vim还有专门的键盘图。。。放一个简略版的 4. 基础命令三剑客三剑客的命令非常之多，完全可以出一本书，这里只放一些简单的和我能用得到的 4.1 三剑客之grepgrep（找基因信息比较方便）Global Regular Expression Print，全局正则表达式版本文本搜索工具，类似于正则表达式搜索，可以在一个大的文件中快速搜索到满足一定规则的内容。 $ grep &quot;&gt;&quot; gene.fna | wc -l # 统计gene.fna文件中序列的条数$ grep -A 2 &quot;3 gi 29732 34486&quot; lastz.axt #将满足条件的行和下面两行显示出来 12grep -E # grep的拓展模式grep -P # 适应perl语言的正则表达式 区分一下：find是搜索目录下满足条件的文件，grep是搜索文件内满足条件的内容 4.2 三剑客之sedsedsed &#x3D; Stream Editor流处理器，数据流过这个工具，格式化成固定的格式sed + 选项参数 + &#39;模式&#39; + 文本或文件 选项参数： 12345-e 替换，并输出到屏幕（搭配重定向）-i 原文件修改-f 根据模式替换-r 拓展的正则表达式-n 输出 模式： 1234g 全局s 替换，一个字符替换另一个d 删除p 打印 输出固定的行$ sed -n &#39;1307p&#39; seq.fna # 输出文件第1307行；$ sed -n &#39;100,200p&#39; seq.fna # 输出文件第100到200行； 替换操作$ sed -e &#39;s/gi/GI/&#39; seq.fna # 将文件中gi全部替换为大写GI；s为替换$ sed -i &#39;s/gi/GI/g&#39; seq.fna # 在原文件上进行替换，并且进行全部替换，g为全局（默认只进行一次替换） 删除操作$ sed -e &#39;/^\\s$/d&#39; seq.fna # 删除文件中的空白行，命令d为删除符合条件的行。\\s为空白；^行首，$行尾$ sed -e &#39;/&gt;/d&#39; seq.fna # 删除包含ref的行，每个ref行都有&gt;$ sed -e &#39;s/:.*//g&#39; seq.fna # 删除冒号之后的所有内容 4.3 三剑客之awkawk也是非常强大的文本处理工具，awk本身也是一门编程语言输出一个列表任意列$ awk &#39;&#123;print $1,$NF&#125;&#39; 1.txt # 输出1.txt的第一列和最后一列 过滤文件结果$ awk &#39;&#123;if ($3&gt;=80 &amp;&amp; $4&gt;=100) print $0&#125;&#39; blast_m8.out # 过滤文件比对结果，将第三列值大于80，并且第四列值大于100的所有结果输出 比较$ awk &#39;$8&gt;$10&#39; input.txt # 输出第8列数值大于第10列数值的行 输出固定行内容$ awk &#39;NR&gt;=20&amp;&amp;NR&lt;=80&#39; input.txt #输出第20到第80行内容 5. 正则表达式正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑 123456789101112131415161718^ 匹配输入字行首 ^eat,识别eat开头的字符串$ 匹配输入行尾 eat$,识别eat结尾的字符串 \\b 单词锚定符 \\beat\\b ,只识别eat字符串. 匹配除“\\n”和&quot;\\r&quot;之外的任何单个字符\\ 转译字符 比如匹配. 则\\.* 匹配前面的子表达式任意次+ 匹配前面的子表达式一次或多次(大于等于1次，例如“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”）# 需要grep -E支持（拓展）? 匹配前面的子表达式零次或一次 # 需要grep -E支持（拓展）[xyz] 字符集合。匹配所包含的任意一个字符x|y 匹配x或y。“z|food”能匹配“z”或“food”。“[z|f]ood”则匹配“zood”或“food”，择译匹配[a-z] 字符范围\\d 匹配所有数字，等同[0-9]\\s 空白，是字符集换页、制表、换行、回车以及空格的简写[\\f\\t\\n\\r]\\w [A-Za-z0-9_]单词包括大小写字母、数字和下划线^ 负值字符范围。匹配任何不在指定范围内的任意字符。（倒三角）\\D 非数字\\W 非字符\\S 非空白字符","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"linux指令","slug":"linux指令","permalink":"http://www.shelven.com/tags/linux%E6%8C%87%E4%BB%A4/"}]},{"title":"shell脚本基本语法总结","slug":"shell脚本基本语法总结","date":"2022-04-19T19:55:58.000Z","updated":"2022-04-20T05:03:08.389Z","comments":true,"path":"2022/04/20/a.html","link":"","permalink":"http://www.shelven.com/2022/04/20/a.html","excerpt":"","text":"简单记录下shell脚本语言的学习 shell脚本运行方式首先要了解什么是脚本，脚本本质上是一个可运行的文件，使用特定格式的指令让系统通过脚本解析器解析并执行你的指令。系统提供的shell命令解析器有sh、bash和ash。可以通过echo $SHELL查看自己linux系统的默认解析方式 shell脚本文件的开头：#!/bin/bash #! 是特殊的用来声明脚本由什么shell解释，否则使用默认shell sh文件有三种执行方式./xxx.sh bash xxx.sh . xxx.sh ./xxx.sh 先按照 文件中#!指定的解析器解析，如果#！指定指定的解析器不存在才会使用系统默认的解析器 bash xxx.sh 指明先用bash解析器解析，如果bash不存在才会使用默认解析器 . xxx.sh 直接使用默认解析器解析 各种引号的区别vim创建脚本文件1111.sh： 123456789#!/bin/bashecho &quot;Phantom的SHELL练习&quot;num=123echo &quot;预设数字=$num&quot;read -p &quot;输入数字&quot; sum # read可以识别标准输入（键盘输入），-p参数设置提示语echo &quot;输出结果=$sum+$num&quot;echo &quot;$sum&quot; # &quot;&quot;解析变量值echo &#x27;$sum&#x27; # &#x27;&#x27;不能解析变量值echo &quot;今天日期`date`&quot; # ``识别为系统命令 变量名不能以数字开头 在变量赋值的过程中，等号两边不能接空格，若要接空格，则整个字符串都要用引号括起来 各种引号区别双引号“”可以解析变量的值单引号‘’不能解析变量的值，包含的变量会被当做字符串反引号`` 反引号的内容作为系统命令并执行 如`date` 各种括号的区别vim创建脚本文件xxx.sh： 1234567#!/bin/bashNum=1000&#123; # 花括号表示在当前shell完成，会影响当前变量 Num=1234 echo &quot;()里面的数字是=$Num&quot;&#125;echo &quot;显示当前shell数字=$Num&quot; vim创建脚本文件xxxx.sh： 1234567#!/bin/bashNum=1000( # 小括号表示在当前shell完成，不会影响当前变量 Num=1234 echo &quot;()里面的数字是=$Num&quot;)echo &quot;显示当前shell数字=$Num&quot; {命令序列} 在当前shell中执行，直接影响当前变量(命令序列) 由子shell完成，不影响当前shell的变量[判断条件]中括号是判断条件，进行数值判断。下面会说明 数值判断vim建立脚本文件xxxxx.sh： 12345678910#!/bin/bashread -p &quot;请输入第一个数字&quot; mread -p &quot;请输入第二个数字&quot; nif [ $m -eq $n ]; then # -eq 判断两个参数是否相等 echo &quot;输入的两个数字相等&quot;elif [ $m -lt $n ]; then # -lt 判断左边参数是否小于右边参数 echo &quot;第一个数字小于第二个数字&quot;elif [ $m -gt $n ]; then # -gt 判断左边参数是否大于右边参数 echo &quot;第一个数字大于第二个数字&quot;fi # if控制语句格式：if elif else fi 数值判断参数详解-eq 比较两个参数是否相等-ne 比较两个参数是否不相等-lt 左边参数是否小于右边参数-le 左边参数是否小于等于右边参数-gt 左边参数是否大于右边参数-ge 左边参数是否大于等于右边参数 字符串提取和替换vim新建脚本文件1234.sh： 1234567#!/bin/bashll=&quot;Phantom Aria f r u i t l e s s l o v e&quot; # 定义字符串变量echo &quot;长度为:$&#123;#ll&#125;&quot; # 字符串长度(包括空格)echo &quot;$&#123;ll:3&#125;&quot; # 从第3个字符往后提取echo &quot;$&#123;ll:3:11&#125;&quot; # 从第3个字符往后提取11个字符echo &quot;$&#123;ll/ /&#125;&quot; # 字符串从左往右删除第一个空格（相当于替换的方式）echo &quot;$&#123;ll// /&#125;&quot; # 删除字符串中所有空格（相当于全局替换的方式） 字符串匹配和删除vim新建脚本文件match.sh： 123456#!/bin/bashll=&quot;Phantom Aria fruitless love&quot;echo $&#123;ll% *&#125; # 从右往左匹配第一个空格，删除右边所有字符串echo $&#123;ll%% *&#125; # 从右往左匹配所有空格，删除右边所有字符串echo $&#123;ll#* &#125; # 从左往右匹配第一个空格，删除左边所有字符串echo $&#123;ll##* &#125; # 从左往右匹配所有空格，删除左边所有字符串 *号是通配符，可以是匹配的任意长度任意字符串%和%%匹配原则：都是从右到左匹配，删除右边，%%称为贪婪匹配#和##匹配原则：都是从左往右匹配，删除左边，##同样称为贪婪匹配，注意通配符位置 for循环语句for循环语句两种写法如下： 123456789for （(初始值；限制值；执行步阶)） #注意两个小括号，少一个都不行do 程序段done或for 变量 in 1 2 3 4 5 6 7 8 9 10 #等价于`seq 1 10`do 程序段done vim建立脚本文件for_example.sh： 12345678#!/bin/bashdeclare -i sum=0 # 强制定义sum为整数型变量（不定义会变成一串字符串）read -p &quot;请输入整数&quot; n # 标准输入定义变量nfor (( i=0; i&lt;=$n; i++ )) # 等同于for i in `seq 0 $n`，不赘述do sum=$sum+$i # 计算0到n之和doneecho &quot;0到这个数的整数之和=$sum&quot; 自定义函数vim建立脚本文件12345.sh： 12345678910111213#!/bin/bashfunction formax()&#123; if [ $n -gt $m ]; then return $n else return $m fi&#125;read -p &quot;输入数值1：&quot; nread -p &quot;输入数值2：&quot; mformax $n $mecho &quot;输入的最大值为$?&quot; # $?表示上个指令的返回值 自定义了一个formax函数判断输入的两个数值大小，可以看出shell脚本中是一行一行读取指令的。自定义函数可以被引用，保存上述{}内的指令至原文件名12345.sh，在下一个脚本文件中，将函数放在脚本开始处， shell解释器发现它才可以进行调用（如下所示） vim建立脚本文件test.sh： 123456#!/bin/bashsource 12345.shread -p &quot;输入数值1：&quot; nread -p &quot;输入数值2：&quot; mformax $n $mecho &quot;输入的最大值为$?&quot; 自定义函数被成功调用","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"shell脚本","slug":"shell脚本","permalink":"http://www.shelven.com/tags/shell%E8%84%9A%E6%9C%AC/"}]},{"title":"转录组数据分析笔记（7）——DESeq2差异分析","slug":"转录组数据分析笔记（7）——R包DESeq2基因差异表达分析","date":"2022-04-18T07:49:20.000Z","updated":"2022-05-08T07:34:47.402Z","comments":true,"path":"2022/04/18/a.html","link":"","permalink":"http://www.shelven.com/2022/04/18/a.html","excerpt":"","text":"前言前面说到DESeq2包需要准备两个输入文件，一个是样本列表矩阵，一个是row count定量表达矩阵，接下来我们要对样本进行两两比对，找到两组之间有多少个基因上调和下调，不进行两两比对直接把4组数据4个重复全部导进去得到的结果是没有意义的 举个栗子，我先进行短日照的“0”组样本和经过“1”天长日照的1组样本之间进行差异基因分析，从前面整理的样本列表矩阵我们可以看到，0组样本四个重复分别是ERR1698194、ERR1698202、ERR1698203和ERR1698204，同样1组数据4个重复分别为ERR1698205、ERR1698206、ERR1698207和ERR1698208，因此我们需要重新整理我们需要的数据做成csv格式，如下所示： 定量表达矩阵第一行需要和样本列表矩阵的第一列一一对应，顺序需要一模一样下面讲解如何使用DESeq2 1. 代码示范前面处理好raw count定量表达矩阵，建立样本列表矩阵后，我们就可以在rstudio里运行DESeq2包进行差异基因筛选了。代码如下。 1234567891011121314151617181920212223library(&quot;DESeq2&quot;)mycounts &lt;- read.csv(&quot;gene_count_matrix_0_1.csv&quot;,row.names = 1)mycounts_1 &lt;- mycounts[rowSums(mycounts) != 0,] # 重新定义数据集，过滤mapping数为0的基因mymeta &lt;- read.csv(&quot;sample_list_0_1.csv&quot;,stringsAsFactors = T) # 载入样本分组文件，遇到字符串将其转化为因子colnames(mycounts_1) == mymeta$id # 检查导入的两个数据集是否匹配，返回值为F需要重新匹配mymeta$index &lt;- factor(mymeta$index,levels = c(&quot;0&quot;,&quot;1&quot;)) # 把样本分组文件的分组列转换到因子，两两比对把对照组放前面！dds &lt;- DESeqDataSetFromMatrix(countData = mycounts_1, colData = mymeta, design = ~index) #构造用于差异表达分析的数据集dds &lt;- DESeq(dds)res &lt;- results(dds)res_1 &lt;- data.frame(res) # 结果res不是常规的数据，需要转化成数据框library(&quot;dplyr&quot;)res_1 %&gt;% # dplyr给数据集增加新列 mutate(group = case_when( log2FoldChange &gt;=1 &amp; padj &lt;=0.05 ~ &quot;UP&quot;, log2FoldChange &lt;=-1 &amp; padj &lt;=0.05 ~ &quot;DOWN&quot;, TRUE ~ &quot;NOT_CHANGE&quot; )) -&gt; res_2table(res_2$group)write.csv(res_2,file = &quot;gene_0_1.csv&quot;, quote = F) # 输出文件 2. 代码详解详细解释一下过程： 在R里运行程序或者写代码，首先要确定好工作目录在哪里，将之前Stringtie转化的定量表达矩阵和样本列表矩阵全都放在工作目录下，这里我的表达量矩阵是transcript_count_matrix_0_1.csv，分组列表矩阵是sample_list_0_1.csv。getwd()可以查看当前工作目录，在全局设置里可以更改工作目录。 library(&quot;DESeq2&quot;) # 加载DESeq2这个R包 mycounts &lt;- read.csv(&quot;gene_count_matrix_0_1.csv&quot;,row.names = 1) # 载入raw count矩阵，以第一列数据作为行名，读取的矩阵命名为mycounts mycounts_1 &lt;- mycounts[rowSums(mycounts) != 0,] # 过滤每一行mapping总数为0的基因，将数据集整理命名为mycounts_1 mymeta &lt;- read.csv(&quot;sample_list_0_1.csv&quot;,stringsAsFactors = T) # 载入样品列表，遇到字符串将其转化为一个因子 colnames(mycounts_1) == mymeta$id # 检查raw count矩阵第一行是否与样品列表的id列是否一致（如下）。这个很重要，不一致跑DESeq2会报错。如果显示false就要调整 mymeta$index &lt;- factor(mymeta$index,levels = c(&quot;0&quot;,&quot;1&quot;)) # 这一步同样重要，把样本分组文件的分组列转换到因子，不然会报错。我这里对照组是第0天，所以把“1”放在“0”之后，这里顺序需要特别说明！样本和定量矩阵的分组只要一一对应可以不排序，这里一定要分清楚哪个组和哪个组进行比较，否则会得出完全相反的结论！ 123dds &lt;- DESeqDataSetFromMatrix(countData = mycounts_1, colData = mymeta, design = ~index) # 中间那一长串是DESeq2包里的函数，countData是raw count定量矩阵，colData是样品列表，design是分组信息，这步是为了构造用于差异表达分析的数据集，并将数据集命名为dds dds &lt;- DESeq(dds) # 分析的核心DESeq程序 res &lt;- results(dds) # 将结果输出至res数据集 res_1 &lt;- data.frame(res) # res不是常规的数据，我们可以用head和class命令查看一下（如下图），需要转化成常规的数据框格式才可以对其进行加减列等操作，转换格式后的数据集名字为res_1 library(&quot;dplyr&quot;) # 加载这个包是为了对数据框进行操作，我是要增加新的一列统计差异表达情况 123456res_1 %&gt;% mutate(group = case_when( log2FoldChange &gt;=1 &amp; padj &lt;=0.05 ~ &quot;UP&quot;, log2FoldChange &lt;=-1 &amp; padj &lt;=0.05 ~ &quot;DOWN&quot;, TRUE ~ &quot;NOT_CHANGE&quot; )) -&gt; res_2 # 调用dplyr包给数据集增加新的一列group，log2FoldChange &gt;&#x3D;1，padj &lt;&#x3D;0.05，判断这个基因表达为上调，在log2FoldChange &lt;&#x3D;-1，padj &lt;&#x3D;0.05时判断这个基因表达为下调，其余情况为该基因表达情况不变。将结果输出到res_2数据集。 FoldChange表示两样品间表达量比值，是倍数变化，差异表达基因分析里，log2 fold change绝对值大于1为差异基因筛选标准。padj是调整后的p值，在p检验里，p值小于0.05是有显著差异的标志。 table(res_2$group) # 查看差异基因表达的结果，上调基因多少，下调基因有多少，不变的有多少 write.csv(res_2,file = &quot;gene_0_1.csv&quot;, quote = F) # 输出和生成gene_0_1.csv文件，即为结果文件 3. 结果演示我对“0”组样本（对照）和长日照1天，2天和3天这一共4组样本分别进行两两对比，做了基因表达差异分析（0_1表示0组和1组对比，1_2表示1组和2组对比，依次类推不再赘述），同时与原文献补充数据2中的差异分析结果做对比 0组与1组对比，187个基因上调，149个基因下调；原文57个基因上调，79个基因下调 0组与2组对比，51个基因上调，42个基因下调；原文21个基因上调，24个基因下调 0组与3组对比，142个基因上调，143个基因下调；原文107个基因上调，84个基因下调 1组和2组对比，26个基因上调，29个基因下调；原文3个基因上调，0个基因下调 1组和3组对比，46个基因上调，50个基因下调；原文8个基因上调，2个基因下调 2组和3组对比，11个基因上调，13个基因下调；原文2个基因上调，1个基因下调 点击这里下载原文基因表达差异总表（补充数据2） 3.1 分析我做的差异分析总体趋势和文章类似，都是短日照组（SD组，也就是0组）与长日照1、2、3天组（LD组，分别对应123组）相比，有较多基因出现差异性表达；而长日照1、2、3天组之间相比差异表达基因较少，这样才合理，表明拟南芥茎尖分生组织在开花期长日照下，确实有不同的基因参与了光周期诱导的开花过程。如果要对这些差异表达的基因做更深入的分析（下游分析），我们就要比对GO库或者KEGG库进行代谢通路富集分析和注释。以后的笔记会说到。至于为什么我得到的差异基因普遍比原文多，一个是差异分析之前用的分析软件不同，在过滤数据过程中我的条件比较松（只过滤了count数为0的基因），另外不同软件的组装、比对和计数的算法实现也不一样。另外，我们得到这个基因差异表达结果之后，还可以做个更直观的火山图看我们的差异基因分布是否合理，之后也会说。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"DESeq2","slug":"DESeq2","permalink":"http://www.shelven.com/tags/DESeq2/"},{"name":"dplyr","slug":"dplyr","permalink":"http://www.shelven.com/tags/dplyr/"},{"name":"R语言","slug":"R语言","permalink":"http://www.shelven.com/tags/R%E8%AF%AD%E8%A8%80/"}]},{"title":"转录组数据分析笔记（6）——HTseq计数定量","slug":"转录组数据分析笔记（6）——HTseq计数定量","date":"2022-04-17T15:49:37.000Z","updated":"2022-04-19T17:06:13.683Z","comments":true,"path":"2022/04/17/b.html","link":"","permalink":"http://www.shelven.com/2022/04/17/b.html","excerpt":"","text":"HTseq也是对有参考基因组转录数据进行表达量分析的，主要用于reads计数。这个软件功能就比较专一，不像stringtie还需要运行prepDE.py脚本进行数据转化，直接一步到位。那为什么我一开始不用HTseq呢？因为我遇到一个bug 主要还是运算速度的问题，我比较了两种定量方式，HTseq定量虽然只有一步，但是速度远不如stringtie，也可能是我的问题，下面会说到。 1. HTseq定量获得raw countvim一个新脚本，输入如下命令： 12345678#!/bin/bashfor i in `seq 194 209`do htseq-count -f bam -s no \\ /media/sf_/data/fastq/bam/ERR1698&quot;$i&quot;.bam \\ # 输入bam文件 /media/sf_/data/ref/Arabidopis_thaliana.gtf # 参考基因组注释文件 &gt; /media/sf_/data/fastq/count/ERR1698&quot;$i&quot;.count # 自定义输出文件done 参数详解-f # 设置输入文件格式，可以是bam或者sam-s # 设置是否是链特异性测序，设置no每一条reads都会和正义链和反义链进行比较 保存运行以后发现这个程序只能分配一个线程（也可能是我没找到分线程的方法），所以可以根据电脑内核数分几个批处理一起运行会快很多（不然就等着干瞪眼&#x3D; &#x3D;）。 还有一点非常重要！bam文件需要提前按照名称排序，不然会出现绝大部分reads mapping不到参考基因组，这种情况会在屏幕上输出提示信息，但是程序还是会继续跑……这时候就别犹豫了赶紧kill这个程序，就算跑完了数据都不能用。可以用samtools sort -n对bam文件进行名称排序，但是排序之后无法再用samtools index建立索引文件，这会导致HTseq运行速度比蜗牛还慢。暂时没找到更好的办法 摊手。 经过漫长长长长长的时间等待，我们可以看看结果文件的head和tail（这里就放一张图吧）： 前面记录了基因名称和mapping上的reads数，最后5行对应不同的mapping情况，在不同的模式下意义不同，官网给出的区别如下图，默认是union模式： 计数结果也可以用multiqc合并，生成在线报告，这里可以直观地看到每个样品比对上的reads数百分比，这里16个样品的比对率都超过80%，说明计数结果都还不错。 2. HTseq结果文件处理HTseq计数定量后得到的是每一个样品的每个基因reads数，我们需要合并每个样品定量数据，手动修改成DESeq2能识别的raw count表达矩阵，还需要再准备一个样本列表矩阵，才能进行后续的DESeq分析。参考一下stringtie最后生成的表达量矩阵文件，我们也需要将HTseq定量结果整理成csv格式（逗号作为分隔符），第一列是基因名，后面是按照样品序列的排序，中间是表达矩阵。 再来看一看HTseq定量生成的文件详情，同样第一列是基因名，后面是raw count数量，^I 表示两列数据是以制表符tab键分隔的，$为换行符。 我的方法比较笨比，除第一个ERR1698194.count文件保留外，其他所有count文件第一列删去并命名为cut.count，然后合并ERR1698194.count和其他所有cut.count文件，再将所有的制表符替换为逗号，最后加上第一行行名和改文件名。 用awk命令删除第一列，写入到新的cut.count文件中： 1234for i in `seq 195 209`do cat ERR1698&quot;$i&quot;.count | awk &#x27;&#123;$1 = &quot;&quot;; print $0&#125;&#x27; &gt; ERR1698&quot;$i&quot;cut.countdone paste组合ERR1698194样品和其他cut.count文件到alldata.count: $ paste ERR1698194.count *cut.count &gt; alldata.count 看看alldata.count的数据格式，列数没有问题，但是awk删除列产生了空格： 用sed命令删除所有空格，替换所有制表符为逗号（两步可以合一步）： $ sed &#39;s/ //g&#39; alldata.count &gt; alldata1.count $ sed &#39;s/\\t/,/g&#39; alldata1.count &gt; alldata2.count 这样就手动生成符合csv格式的文件了，只需加上第一行： 这里样本量比较少，我直接vim复制粘贴的方法加了第一行，重命名一下文件就完成了表达矩阵的制作，可以用于DESeq2分析了！ 因为本人比较小白，上面处理过程就有些啰嗦了，总的思路就是改成csv格式文件的样式就可以。 样本列表矩阵的制作过程和stringtie一模一样，点击这里查看，本篇不再赘述。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"HTseq","slug":"HTseq","permalink":"http://www.shelven.com/tags/HTseq/"}]},{"title":"转录组数据分析笔记（5）——stringtie转录本组装和定量","slug":"转录组数据分析笔记（5）——stringtie转录本组装和定量","date":"2022-04-17T15:14:21.000Z","updated":"2022-04-25T07:09:01.626Z","comments":true,"path":"2022/04/17/a.html","link":"","permalink":"http://www.shelven.com/2022/04/17/a.html","excerpt":"","text":"stringtie转录本组装和定量1 转录本组装Stringtie是一个基因和转录本组装定量的软件，stringtie的输入文件有两个，一个是经过排序的bam文件，排序可以用前面说到的samtools sort命令完成，还有一个是参考基因组的注释文件（gff或gtf格式）。 在使用Stringtie进行基因或者转录本组装定量的过程中，有一个非常重要的参数 - e，我之前跑了一遍流程没有加参数-e，结果组装的结果非常差，还有大量的未注释的基因。我请教了度娘，网上的教程攻略也都是抄来抄去的没解决什么问题，官网只有这么一句解释： -e this option directs StringTie to operate in expression estimation mode; this limits the processing of read alignments to estimating the coverage of the transcripts given with the -G option (hence this option requires -G). 对于加了参数-e之后如何做的比对和组装处理还是不明了，不知道表达评估模式的原理是什么，只能自己做个大概的总结（不知正确与否）： 如果我们研究的样本没有很好的注释信息，研究的人少，现有的注释信息都不完善，那么我们就需要重建转录本进行注释，这个时候就不需要加参数-e。如果样品的注释信息非常完整，比如拟南芥这种模式生物，我们不需要重建新的转录本进行注释，只对现有的参考基因组注释文件就足够了，那就要用-e参数，不需要预测新的转录本。 -e参数还有个比较重要的地方，只有用了-e参数后，才可以运行prepDE.py3脚本得到read count矩阵（也就是进行定量），这个脚本后面会说。 我们首先创建一个shell脚本进行转录本组装： 12345678#!/bin/bashfor i in `seq 194 209`do stringtie -p 4 -e \\ -G /media/sf_/data/ref/Arabidopis_thaliana.gtf \\ # 参考基因组注释文件 -o /media/sf_/data/fastq/gtf/ERR1698&quot;$i&quot;.gtf \\ # 自定义输出文件 /media/sf_/data/fastq/bam/ERR1698&quot;$i&quot;.bam # 输入的bam文件done 保存，运行，我们可以得到.gtf格式文件，less一下查看里面的内容： 我们这里因为加了参数-e，不会有新的基因和转录本，可以看到每个read比对上的基因的信息。（不加参数-e会组装新基因和转录本，默认采用STRG加数字编号进行区分）。每行数据会给出coverage，FPKM和TPM三个信息，后两者都可以用来定量。FPKM和TPM都是对read counts数目进行的标准化，如果是单端测序数据可以用RPKM进行标准化，不进行数据标准化的比较是没有意义的。 2 合并转录本(重构转录本才需要)这一步要注意下，如果需要重构转录本才需要合并所有的转录本的组装结果，得到一个非冗余的转录本合集，也就是获得跨多个RNA-seq样品的全局的转录本。这里需要分两步： $ ls *.gtf &gt; mergelist.txt # 将所有组装的转录本文件名合并到一个文件 $ stringtie --merge -p 4 -G /media/sf_/data/ref/Arabidopis_thaliana.gtf -o merge.gtf ./mergelist.txt #这一步是用--merge指令将所有转录本合并输出到merge.gtf文件中 我们最后得到的merge.gtf就是全局的转录本。这里只是记录一下这步操作，我们只关注参考基因组的注释结果就不需要merge。 3 获得定量表达矩阵DESeq2要求输入的定量结果为raw count形式，raw count是根据mapping到基因或转录本的reads数计算得到，而stringtie只提供了转录本水平的表达量，定量方式包括TPM和FPKM值两种。为了进行raw count定量，stringtie官方提供了prepDE.py脚本（两个版本，我选择的python 3版本，在我base环境下不会冲突），可以计算出raw count的表达量。 下载这个python脚本，如果你用的是windows浏览器，在官网找到脚本直接右键复制链接，用wget直接下到linux系统里，千万不要在windows上直接复制粘贴代码过去。因为windows的换行符和linux的不一样，两个系统间直接粘贴代码会出现错行和莫名其妙的缩进导致程序报错（可以用cat -A看两个系统换行符的区别，血的教训，排查了老半天才发现）推荐用prepDE.py3，不用再切python 2 的环境了。 官方给出的prepDE.py脚本有两种运行方式（如下图所示），一种是建立Ballgown能识别的目录结构，一种是建立sample_lst文件并指定所有样品数据的路径。两种方法都可行，Ballgown现在用的比较少，比较主流的还是Stringtie+DESeq2的分析方法。演示一下如何创建sample_lst和解释一下这个文件要求的格式。 3.1 sample_lst文件准备简单来说，sample_lst.txt要求第一列为样品编号，第二列为对应编号的样品gtf文件所在路径，中间用制表符tab隔开，如下图（命名不一定要完全一样，注意格式，后面要导入prepDE脚本，能找到就行）： 这个文件准备工作比较简单，不再赘述 3.2 运行prepDE.py3将prepDE.py3脚本放在上面gtf文件的目录下，运行以下命令： $ python prepDE.py3 -i sample_lst.txt -g gene_count_matrix.csv -t transcript_count_matrix.csv 解释一下： 参数含义-i # 输入文件，就是前面做的sample_lst.txt-g # 自定义基因组表达矩阵名字，默认也是gene_count_matrix.csv-t # 自定义转录本表达矩阵名字，默认也是transcript_count_matrix.csv 得到的这两个文件就是基因和转录水平的raw count表达量矩阵，我们都可以用于后面的DESeq2分析。 4. 制作样本列表矩阵这里需要和前面为了运行prepDE.py脚本而制作的sample_lst文件区分开，要做下一步DESeq2差异基因分析，我们需要自己手动创建一个DESeq2能识别的样本列表矩阵，包含两列信息：一列是样本名称，一列是样本分组。样本分组信息我们可以直接从下载样本数据的地方（EBI官网）得到，只需要自己改一下格式。 下载之后发现第一行标题特别长，稍微处理下制表符替换成换行符，将第一行标题拆分成每个字段一行的格式，找一下不同天数处理的分组信息关键字“time”，发现我们要的分组信息在第36行（也就是原来文件的第36列）： $ head -n1 E-MTAB-5130.sdrf.txt | tr &#39;\\t&#39; &#39;\\n&#39; | nl | grep &quot;time&quot; 同样的方法找样本信息所在列是32列： $ head -n1 E-MTAB-5130.sdrf.txt | tr &#39;\\t&#39; &#39;\\n&#39; | nl | grep &quot;ENA&quot; 所以我们需要提取第32列和第36列，用cut命令切割并重定向到新的文件sample_list: $ cut -f 32,36 E-MTAB-5130.sdrf.txt &gt; sample_list.csv 发现相邻数据有重复，uniq删除重复行，再用sed替换制表符为逗号（因为csv文件就是以逗号作为分隔符），将原来的sample_list.csv覆盖，vim手动修改一下第一行名字，完成后就可以用于DESeq2分析了！ $ uniq sample_list.csv &gt; sample_list1.csv # uniq删除重复行 $ sed &#39;s/\\t/,/g&#39; sample_list1.csv &gt; sample_list.csv # 替换制表符为逗号 手动修改sample_list .csv第一行内容，修改之后如下即可 更新2022&#x2F;4&#x2F;22：这里的处理仅仅是做到符合DESeq2输入的格式，在进行两组样本基因表达差异分析的时候，还需要分别建立两两比对的样本列表矩阵和定量矩阵，不然差异分析没有意义，详见DESeq2笔记","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"stringtie","slug":"stringtie","permalink":"http://www.shelven.com/tags/stringtie/"},{"name":"prepDE.py3","slug":"prepDE-py3","permalink":"http://www.shelven.com/tags/prepDE-py3/"}]},{"title":"转录组数据分析笔记（4）——IGV基因组浏览器安装和解读","slug":"转录组数据分析笔记（4）——IGV基因组浏览器安装和解读","date":"2022-04-16T11:32:50.000Z","updated":"2022-04-16T11:37:33.011Z","comments":true,"path":"2022/04/16/b.html","link":"","permalink":"http://www.shelven.com/2022/04/16/b.html","excerpt":"","text":"1. IGV软件下载IGV（Integrative Genomics Viewer）是一个非常方便的比对软件，在使用前只需要将参考基因组和bam文件分别建立索引（即建立fai和bai文件）并载入，就可以对转录组测序数据进行可视化浏览。对比samtools tview功能，这个软件有交互式操作界面，对萌新非常友好。 直接上百度搜就能找到IGV官网，选择linux版本或者windows版本都行，这里用linux版本为例，IGV只支持JAVA11版本，不用担心这个问题，下载的安装包里直接有JAVA11，解压就可以用，就是国外网站下载有点慢（科学上网）。 直接在虚拟机里解压打开，运行igv.sh，会自动准备好JAVA11的运行环境，成功弹出交互式界面（终于告别了黑漆漆的命令行 ）。 2. 导入文件Genomes菜单栏上传建立索引的参考基因组.fa和.fai文件： File菜单栏上传排序并建立索引的.bam和.bai文件： 如果有参考基因组注释文件，同样可以导入进去，同样导入前需要sort排序和建立index，可以用菜单栏里的igvtools直接sort和index： 3. 界面解读我导入了5组bam数据，所有文件导入后可以看到如下界面，简单介绍一下各个区域和功能： 主页面获得的信息有限，我们选取第3条染色体为例，将其放大： 中间的界面可以通过左右拖动鼠标，或者按左右方向键来浏览染色体上的比对情况。我们在搜索框中直接搜基因名字进行染色体定位，比如CIPK家族中的CIPK7基因，回车后双击最后一栏基因注释文件中的基因名称CIPK7，可以得到详细的CIPK7基因信息（这里注意下，如果双击弹出来多个可供选择的片段的话，代表这个基因存在可变剪切）: 在基因注释区右键，选择expanded，可以将CIPK7基因的所有转录本显示出来。 放大到一定程度后，我们可以看到基因注释区上方出现了核苷酸序列和氨基酸序列，我们可以点击sequence旁边的箭头，切换到另一条链的序列。 点击核苷酸，会出现三行，分别表示不同起始位点的核苷酸翻译结果，绿色为起始密码子，红色的星号表示终止密码子。 再来看看放大后的tracks区域，bam文件在载入后会默认生成两个tracks，一个显示测序深度（Coverage track，可以对比下samtools depth），一个显示比对情况（Alignment track），我们放大其中一个样本的数据信息。 Coverage track区域灰色代表质量好，如果reads中某核苷酸与参考序列超过20%不一致的时候，IGV会根据四个碱基的计数对coverage的条形图进行着色。这里可以看到该位点处有20个reads覆盖到，8条reads测的是C核苷酸，12条reads测的是T核苷酸。如果某个位置coverage条形图只有一种颜色，即该位点测的核苷酸和参考序列完全不一样，那说明该位点是SNP位点。 Alignment tracks柱形图是和bam文件中的数据一一对应的，举个例子，我在IGV软件的ERR1698206.bam可以看到在第3条染色体位置8173028有3条reads。虚拟机中找到这个bam文件，直接samtools view查看并grep这个位置，可以找到3条定位的reads（还有三条是配对的另一条链）。 如果一条reads中间有缺失，IGV会用黑色横杠表示，中间数字表示缺失几个核苷酸。 IGV还用不同颜色标记异常的插入片段大小的reads，这里做的是RNA-seq数据比对，不用看reads颜色，有些reads还在质控的时候被裁短了，变成蓝色很正常（因为比预期短，个人理解是这样，有待考证？）。以下是官网的默认着色方案：","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"IGV","slug":"IGV","permalink":"http://www.shelven.com/tags/IGV/"}]},{"title":"转录组数据分析笔记（3）——samtools用法小结","slug":"转录组数据分析笔记（3）——samtools用法小结","date":"2022-04-16T10:41:24.000Z","updated":"2022-04-20T08:15:24.059Z","comments":true,"path":"2022/04/16/a.html","link":"","permalink":"http://www.shelven.com/2022/04/16/a.html","excerpt":"","text":"1. sam文件转化bambam文件是二进制文件，占用磁盘空间小，运算速度快，samtools操作是针对bam文件的，所以我们要进行数据转化。samtools sort指令可以将bam文件进行排序，这个指令同时也可以将sam文件转化成bam文件： 12345#!/bin/bashls *.sam | while read iddo samtools sort -l 0 -@ 5 -o $(basename $id &quot;.sam&quot;).bam $id # 指定输出文件，改后缀.bamdone 运行脚本，将当前目录的sam文件全转换成bam文件并排序（这里的排序不是按名称，用HTseq还要再按照read名称排序，使用参数-n）。 samtools sort参数samtools sort # 对bam文件进行排序（sam文件排序不会变）-l # 设置输出文件压缩等级，0-9，0是不压缩,9是最高等级压缩-@ # 设置线程数-o # 设置排序后输出的文件名最后接输入的bam或者sam格式文件 2. 构建索引文件2.1 构建bam文件索引在bam文件目录下，排序后的bam文件可以建立索引： $ ls *.bam | xargs -i samtools index &#123;&#125; 注意下xargs -i的用法，和管道不一样，是传递参数给后一个命令的花括号中，后一个命令中不存在歧义的时候可省略参数-i和花括号。 如图生成的bai文件就是索引文件。其实到了这一步，前面的sam文件就可以删除（节省电脑空间），只留下bam文件就行，bam文件无法直接查看，可以通过samtools view命令查看bam文件。 2.2 构建参考基因组fa文件索引在参考基因组文件目录下，对参考基因组的fa文件建立索引： $ samtools faidx Arabidopsis_thaliana.dna.genome.fa 参考基因组文件名注意改成自己的，生成的索引文件是.fai结尾的 3. bam文件qc质控samtools转化生成的bam文件需要进行质控，看看比对情况如何。在bam文件目录下，我们创建一个samtools自带qc质控指令samtools flagstat运行脚本： 12345#!/bin/bashls *.bam | while read iddo samtools flagstat -@ 4 $id &gt; $(basename $id &quot;.bam&quot;).flagstat # 自定义输出文件done $ samtools flagstat bam文件 &gt; 输出文件 # 这种格式，其他参数都一样 运行脚本文件可以获得16个.flagstat质控文件，和fastqc一样，我们还可以做完后用multiqc命令集合成一个html格式的总的qc报告网页。和fastqc不同之处是，fastqc是做下机数据质控，samtools是做比对参考基因组的质控。如下图所示，可以比较直观地看出大部分reads都是map上的。 生成的每一个flagstat文件我们也可以直接点开。 每一行统计数据都是以通过QC的reads数量和未通过QC的reads数量组成，以我点开的这个文件为例，主要信息有以下几个： 13992629个reads都是合格的12328290个reads只比对到参考基因组一个位置上13988737个reads比对到参考基因组（99.97%）12332182个reads是成对的12201338个reads可以正确配对（98.94%）2846条reads成对但只有一条能比对上参考基因组12398个配对的reads可以比对到别的染色体上 可以自己将所有的flagstat运行结束后的文件放在一个目录下，运用paste命令全部按列粘贴在一起，用cut或者awk提取所需的列数据自己做比对情况表格，这里不再赘述。 4. samtools其他指令简单介绍一下： $ samtools view ERR1698194.bam #查看bam文件（不能直接cat查看二进制文件） $ samtools tview ERR1698194.bam #类似于IGV这种基因组浏览器，但是非交互式界面（下图）不直观，我们一般都是用IGV查看基因组 其他还有samtools merge（合并所有bam文件到一个文件），samtools depth（得到每个碱基位点或者区域的测序深度,并输出到标准输出）等等，不是特别常用，这里就不介绍了。 在步骤2中构建的索引文件可以导入IGV中，对转录组每个read mapping情况进行可视化浏览，下个笔记将介绍IGV的用法。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"samtools","slug":"samtools","permalink":"http://www.shelven.com/tags/samtools/"}]},{"title":"转录组数据分析笔记（2）——使用Hisat2比对参考基因组","slug":"转录组数据分析笔记（2）——使用Hisat2比对参考基因组","date":"2022-04-15T08:53:41.000Z","updated":"2022-05-23T18:44:17.679Z","comments":true,"path":"2022/04/15/a.html","link":"","permalink":"http://www.shelven.com/2022/04/15/a.html","excerpt":"","text":"1. 建立参考基因组索引在进行clean data与参考基因组比对之前，我们需要先建立参考基因组索引。进入下载好参考基因组的文件目录下，运行命令： $ hisat2-build Arabidopsis_thaliana.dna.genome.fa genome -p # 以几个线程运行，与电脑核数或者分配虚拟机的核数有关genome # 命名的索引文件名，可以改成自己能找到的 就可以在当前目录建立参考基因组索引文件，hisat2固定会生成8个以.ht2做后缀名的索引文件，如下所示： 需要注意的一点，比对软件除了hiasat2以外，还有subjunk、bwa、bowtie2等等，各个比对软件生成的索引文件是不同的，不能相互混用，命名的时候注意区分各种比对工具。 2. clean data与参考基因组比对比对的意思是将每一个read与参考基因组序列进行对比，目的是得到每一个read在参考基因组上的位置信息，有了这个基础的位置信息才可以进行后续基因或者转录本的定量，最终由定量结果做差异表达矩阵，分析上调或者下调的基因数量。 新建一个shell脚本输入下面的代码 12345678#!/bin/bashfor i in `seq 194 209`do hisat2 -p 4 -x /media/sf_example/data/ref/genome \\ #索引文件绝对路径 -1 /media/sf_example/data/clean_data/ERR1698&quot;$i&quot;_1.fq.gz \\ -2 /media/sf_example/data/clean_data/ERR1698&quot;$i&quot;_2.fq.gz \\ -S /media/sf_example/data/hisat2_sam/ERR1698&quot;$i&quot;.sam #注意大写的Sdone 参数解释： -p # 同样是配置线程数-x # 指定索引文件，需要定义索引文件名称，不能加后缀，不能只定义到索引文件所在目录-1 # 第一端测序数据文件-2 # 第二端测序数据文件-S # 指定输出目录和文件，不指定会刷屏，注意是大写的S 输出到屏幕的结果如下，我们选取其中一个进行解读： 共有6166091对测序数据，都是双侧测序数据，其中： read1 和 read2 没有合理比对上参考基因组序列的有65259对，占1.06% read1 和 read2 只有一条比对上参考基因组序列的有5698903对，占92.42%，这部分reads数需要占测序reads的绝大多数才正常 read1 和 read2 可以同时比对到多个地方的有401929对，占6.52% 65259对没有合理比对上的序列中，55871对可以不合理地比对上一次 最后一块是对两条链拆开比对的结果，这个一般用不到，本来测序的两条reads就应该比对到同一个染色体同一个基因附近，拆开比对到不同染色体没有意义。我们要看的是最后一句话，总比对率为99.97%，通常比对率大于90%说明比对情况较好，与参考基因组基本吻合。 3. sam文件解读比对结果除了有屏幕上输出的总体报告外，还有记录详细比对结果的sam文件。双端测序的比对会将两个测序文件进行整合和比较，最后只生成一个sam文件，因此这个sam文件非常大，hisat2比对生成的sam文件可以直接打开。我们可以选取一部分进行解读。 @HD VN:1.0 SO:unsorted （排序类型） VN是格式版本；SO表示比对排序的类型，有unknown，unsorted，queryname和coordinate几种。samtools软件在进行行排序后不能自动更新sam文件的SO值。 @SQ SN:1 LN:30427671 （序列ID及长度） 参考序列名，这些参考序列决定了比对结果sort的顺序，SN是参考序列名；LN是参考序列长度；每个参考序列为一行。这里表示拟南芥有5条染色体，对应长度都在后面，Mt是线粒体基因，Pt是叶绿体基因。 @PG ID:hisat2 PN:hisat2 VN:2.2.1 （比对所使用的软件及版本） 这里包括了路径，方法，以及我质控后的序列长度（50-100）等详细信息。 接下来每行都是一长串，显示的是比对结果部分，11个字段（列） 第一列：QNAME：测序出来的reads序列数据名，ERR1698194.2 第二列：FLAG：表明比对类型：paring，strand，mate strand等 第三列：RNAME：参考基因组的染色体名，我这里是第1条染色体 第四列：POS：比对到这个染色的具体位置，4969 第五列：MAPQ：比对质量，是一个衡量比对好坏的打分结果，60最好 第六列：CIGAR：简要比对信息表达式，1S100M是第1个碱基切除，100个匹配 第七列：RNEXT：另一个序列比对上的参考序列编号，没有另外的片段是*，同一个片段&#x3D; 第八列：MPOS：另一个序列匹配的染色体具体位置，这里一样也是4969 第九列：TLEN：配对片段长度，最左边的为正，最右边的为负 第十列：SEQ：和参考序列在同一个链上比对的序列 第十一列：QUAL：比对序列的质量和reads碱基质量值 后面提供额外的信息，一般不重要，了解一下就行。因为sam文件太大（往往有10G以上），也不适合电脑进行后续处理，所以我们会用到samtools，将sam文件转化为更适合电脑处理的二进制bam文件。这个后面会讲。 4. 其他比对软件以下4种软件均用于序列比对，用法稍有不同，做个记录 $ hisat2 -p 4 -x 索引目录 -1 单端测序数据文件 -2 另一端测序数据文件 -S 输出文件 $ subjunk -T 4 -i 索引目录 -r 单端测序数据文件 -R 另一端测序数据文件 -o 输出文件 $ bowtie2 -p 4 -x 索引目录 -1 单端测序数据文件 -2 另一端测序数据文件 -S 输出文件 $ bwa mem -t 4 -M 索引目录 单端测序数据文件 另一端测序数据文件 &gt; 输出文件","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"hisat2","slug":"hisat2","permalink":"http://www.shelven.com/tags/hisat2/"}]},{"title":"转录组数据分析笔记（1）——如何用fastqc和trim-galore做测序数据质控","slug":"转录组数据分析笔记（1）——如何用fastqc和trim-galore做测序数据质控","date":"2022-04-14T13:13:35.000Z","updated":"2022-05-23T18:37:31.014Z","comments":true,"path":"2022/04/14/a.html","link":"","permalink":"http://www.shelven.com/2022/04/14/a.html","excerpt":"","text":"本系列学习笔记数据均来自”Temporal dynamics of gene expression and histone marks at the Arabidopsis shoot meristem during flowering“，原文用RNA-Seq的方式研究开花阶段，芽分生组织不同时期的基因表达量变化，4个时间段（0, 1, 2, 3），4个重复，共有16个样品。点击这里获取文献 1. 读文章获得RNA-Seq数据从文章末尾我们可以获得一些测序数据信息： Data availability. ChIP-seq and RNA-seq data have been deposited with ArrayExpress database (www.ebi.ac.uk/arrayexpress), accession numbers E-MTAB-4680, E-MTAB-4684 and E-MTAB-5130. 可以看到作者将CHIP-seq和RNA-seq数据上传到ArrayExpress这个数据库中，这个数据库是欧洲生物信息研究所（European Bioinformatics Institute, EBI）旗下的公共数据库，主要用于存放芯片和高通量测序数据，我们可以直接从该数据库中下载我们需要的RNA-seq数据，自己动手分析。 顺便介绍一下，欧洲EBI旗下的ENA数据库，美国NCBI旗下的GenBank，以及日本的DDBJ三大巨头组成了国际核酸序列数据库合作联盟（INSDC），这三大数据库收录了世界上报道的所有序列数据。 EBI数据库可以直接下载fastq数据，不需要做SRA数据转换（NCBI数据库中下载sra数据则需要转换，需要用工具fastq-dump），这是EBI数据库下载高通量测序数据的优点，但是这个数据库经常网络连接不稳定，用aspera或者prefetch这种高速下载软件也不一定能稳定下载 最好的方法是科学上网。我们可以从ArrayExpress数据库中输入索引号E-MTAB-5130，直接获得样本信息和测序信息。 2. 测序数据质控我们可以看到，下载的数据是双端测序产生的。我们不能直接用下载的raw data做后续分析，必须要进行质控查看测序质量如何。 2.1 使用fastqc对测序数据生成质控报告下载好的fastq文件可以直接用fastqc工具做测序数据质控，输入以下命令一次生成所有qc报告： $ fastqc *.fastq.gz -o ./ #在当前目录下对所有.fastq.gz文件生成qc报告，-o参数定义输出目录 运行结束后我们可以得到.html文件和.zip压缩包，这个就是质控报告。在虚拟机里，我们可以直接点开.html后缀的网页文件查看质控报告（和压缩包的内容是一致的）。 顺便介绍一个非常好用的工具multiqc，可以通过conda install直接安装，这个工具可以将批量生成的qc报告合并为一个，看起来更加直观。在生成qc报告的当前目录下，运行代码： $ multiqc ./ 2.2 质控报告解读2.2.1 基本信息绿色表示通过，黄色表示不太好，红色表示不通过。RNA-seq一般在Sequence Duplication Levels上结果会不好，一个基因可能会大量表达，测到好多遍。 2.2.2 核苷酸测序质量箱式图这里测序质量（纵坐标）用Q值表示，p为出错率，Q值计算式为Q&#x3D;-10*lg（p）。每一个核苷酸的测序质量可以从fastq文件第四行一一对应上，这里只是做了统计和可视化。我们可以看到每个位点的核苷酸测序质量Q值都在30以上，意味着每个位点的测序正确率都在99.9%以上，可以认为测序质量比较好。 箱式图解读：黄色箱子(25%和75%的分数线)，红色线(中位数)，蓝线是平均数，下面和上面的触须分别表示10%和90%的点。 2.2.3 测序泳道质量图纵坐标为tile编号，这张图代表每次荧光扫描的质量。蓝色背景表明测序质量良好，白色和红色的背景表示测序过程中可能有小气泡或者测序泳道上有污染。直接的体现就是部分测序数据中出现连续的N，也就是不能读取，可能是任何一个核苷酸。 2.2.4 reads质量得分可以看到平均质量在38，质量比较高。如果最高峰所对应的横坐标质量值小于27（错误率0.2%） 则会显示“警告”，如果最高峰的质量值小于20（错误率1%）则会显示“不合格”。 2.2.5 每条reads各个测序位点上各碱基出现概率图上看得出比较稳定，测序刚开始的时候波动会大一点，这里的GC含量和AT含量不一致。如果任何一个位置上的A和T之间或者G和C之间的比例相差10%以上则报“警告”，任何一个位置上的A和T之间或者G和C之间的比例相差20%以上则报“不合格”。 2.2.6 GC含量和理论分布可以看出GC含量在43%左右，与理论分布（也就是正态分布）比较吻合，中心峰值与所测转录组的GC含量一致。如果有不正常的尖峰，可能是测序文库有污染，接头的污染还会在过表达序列中体现。 2.2.7 每条reads的含N碱基数不能识别的碱基会被读成N，这里没有N，测序质量非常好。横坐标表示reads的位置，纵坐标表示N的比例。如果任何一个位置N的比例大于5%则报“警告”，大于20%则报“失败”。 2.2.8 测序长度分布这个测序仪一次测量长度是101bp。测序仪出来的原始reads通常是均一长度的，经过质控软件处理过的reads长度则不一样，这里说明测序结果较好。 2.2.9 重复序列水平可以看到重复水平较低。图中横轴代表reads的重复次数，大于10次重复后则按不同的重复次数合并显示。纵坐标表示各重复次数下的reads数占总reads的百分比；蓝线展示所有reads的重复情况，红线表示在去掉重复以后，原重复水平下的reads占去重后reads总数的百分比；如果非unique的reads占总reads数的20%以上则报 ”警告“，占总read数的50%以上则报 ”不合格“。这项变黄是正常的。 2.2.10 过表达序列和接头序列过表达的序列很可能是一些测序的接头序列，这里两种序列都看不到，说明质量良好。过表达序列是显示同一条reads出现次数超过总测序reads数的0.1%的统计情况，超过0.1%则报“警告”，超过1%则报“不合格”，会列出可能的接头序列。接头序列正常情况下含量接近于0。 2.3 trim-galore测序数据质控过滤质控的目的使为了除去下机数据raw data中的接头序列和质量比较差的测序数据，Q&lt;20，正确率小于99%，如果这样的核苷酸超过read长度的20%，则考虑将该read丢弃（只是建议，不是强制，根据需要可以自定义过滤条件）。 trim-galore也可以用conda install安装，非常方便，这是一个自动检测adaptor的软件，可以一个命令自动找出主流的测序接头并去除，还可以设置参数对测序数据质控。简单介绍一下trim-galore的一些参数： -q # 设定Phred quality score阈值，默认为20；-phred33 # 测序平台衡量测序质量的方法，有33和64，不影响；-length # 设定输出reads长度阈值，小于设定值会被抛弃，根据需要设计;-stringency # 设定可以忍受的前后adapter重叠的碱基数，默认为1（非常苛刻）;-paired # 用于分析双端测序数据结果；-o # 输出目录 因为是双端测序，16个样本每个都有_1和_2两个文件，可以写个脚本批量运行： 12345678#!/bin/bashfor i in `seq 194 209` do trim_galore -q 25 -phred33 -length 50 -stringency 3 -paired \\ -0 /media/sf_/example/data/clean_data \\ /media/sf_/example/data/raw_data/ERR1698&quot;$i&quot;_1.fastq.gz \\ #一端测序数据 /media/sf_/example/data/raw_data/ERR1698&quot;$i&quot;_2.fastq.gz #另一端测序数据done 保存退出，运行，最后生成的_triming_report.txt文件就是生成的质控报告，_val_1.fq.gz就是过滤后瘦身的clean data，我们可以看到大小比原来小了10M左右，这个clean data才可以用于后续的分析流程 我截取了其中一个数据的质控结果，拉到最底下，可以看到两端测序数据中都有AGATCGGAAGAGC这个序列，在一个样本测序数据中出现240027次经过网上查找，AGATCGGAAGAGC这个序列确实是Illumina公司测序时的接头序列（点击这里查看），可以和上面fastqc质控报告中的测序平台Illumina相互验证。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"fastqc","slug":"fastqc","permalink":"http://www.shelven.com/tags/fastqc/"},{"name":"multiqc","slug":"multiqc","permalink":"http://www.shelven.com/tags/multiqc/"},{"name":"trim-galore","slug":"trim-galore","permalink":"http://www.shelven.com/tags/trim-galore/"}]},{"title":"小破站正式对外开放啦！","slug":"小破站正式对外开放啦！","date":"2022-04-13T14:29:14.000Z","updated":"2022-05-23T18:22:10.594Z","comments":true,"path":"2022/04/13/a.html","link":"","permalink":"http://www.shelven.com/2022/04/13/a.html","excerpt":"","text":"前言咳咳，经过10天左右紧张地准备，小站今天正式对外开放啦！作为第一次运行个人网站的小白，看着网站从零开始在自己手上慢慢展现一个个页面，实现一个个功能，这种成就感和激动感，让我感觉这几天的熬夜狂肝还是值得的呜呜呜我的头发。 建站过程为什么建站说是从零开始，其实也是站在前人搭建好的框架上才能顺利进行的。我很早之前就萌发了搭建个人网站的想法，自从这个学期开始学习生物信息学，我也慢慢对linux系统有了更深入的理解。一开始只是在虚拟机上跑跑程序，后来就想着不如买一个云服务器装linux玩玩，既然买了服务器了，那就再绑个域名吧，既然两个都有了，不如就再建个网站吧（滑稽）。于是趁着腾讯云的轻量级应用服务器打折的机会，一次性买了3年…然后又在阿里云买了个域名，了解到需要备案后才能解析域名，行，又办理各种手续在工信部备了案。不得不说，在各大云服务器商内卷搞活动的时候，有个学生认证还是相当香的。至于怎么用服务器，那就是后面考虑的事了。 备案和备案期间的学习我在腾讯云买的服务器，通过接入商腾讯云协助，腾讯云先审核我的材料，通过以后再提交工信部备案，备案还是相当快的，3天时间就办下来了。备案期间也没闲着，作为一个前端小白的我，又去恶补了一些前端知识，比如什么是css、js、ejs、html文件，这些文件的格式是怎么样的，java的一些基本语法等等。学习的折磨程度不亚于刚开始学R语言和linux操作系统，不过有了一些shell脚本的语法知识以后，还是能感觉到这些语言之间还是有共同的判断方式和逻辑在里面的（纯小白发言，不知道对不对）。在慢慢摸索的过程中痛并快乐着，先是照着别人给的js文件魔改，再是自己调试遇到的问题和bug，尤其在发现bug最后解决bug的时候，那种成就感能给我带来莫大的快乐。 建站历程建站的过程是痛苦的，踩了非常多的坑，我觉得我甚至可以写好几篇攻略出来。我一开始的想法是在github建库搭建个人网站，从安装nodejs和npm这种最基础的开始，配置环境，用hexo框架搭建一个本地静态博客，然后部署到github空间，这样就可以用github仓库名访问我的网站。但是有一个非常大的问题，github从国内访问会有DNS污染，连接速度那叫一个绝望。我自己是可以科学上网，但是总不能让别人浏览我网页的时候也科学上网吧？我也不太相信有很多人会用改host的办法来访问github，于是我就萌生了将买的云服务器用来搭建网站的想法（我知道这是一种资源浪费），github就可以当做网站的备份，以后即使我的云服务器过期了，我也可以依旧正常访问搭建在github里的静态博客。所以我的部署过程有点绕，就是本地生成静态博客，先部署到github仓库，再同步部署到我的云服务器。这样我就可以用备案后的二级域名解析到云服务器，在通过安装httpd服务来开启外部的访问了。 可以访问我的网站还是第一步，还要做好安全防护，申请SSL安全证书才能开启https连接。免费申请方式也很多，我申请了一年的apache上的SSL安全证书，然后安装到自己服务器上。还想吐槽一下，腾讯云有一键部署SSL安全证书通道，要90块钱，只要有点linux文本操作基础，自己按照教程部署一下半小时左右就能完成，这钱真好赚。SSL证书安装做好以后，就可以上别的云服务商找找免费的CDN加速了，有CDN加速一是可以加快网站的加载速度，二是隐藏自己服务器的ip地址，能起到一定的网站安全防护作用。吹一波又拍云，只要在网站底下加上他们的标志，启用他们的CDN加速，就能申请加入又拍云联盟，有免费一年的CDN加速和云储存服务，还可以查看访问记录等等 学生党薅羊毛的利器23333 。因为我的网页图片比较多，所以就应用了网页图片加速。 后记具体过程比如怎么接入第三方各种网站，用什么主题，怎么美化页面等等，就不详细说了，说多了肝疼，以后有想法再更新如何从零开始搭建自己的博客吧！至少没有服务器和域名也是完全可以实现的。建立这个小破站也主要是为了上传自己的学习笔记，整理生信网站和工具合集（相应的栏目还在建设中 新建文件夹了 ），督促自己学习hhhhh 本人技术实力有限，也不想搞地太花里胡哨，之后可能会有一些简单的小功能接入，还有移动端浏览小破站的优化（现在移动端浏览这个小破站简直是灾难，我都看不下去了），太费心思的东西就暂时放放了，主要专注于内容的创作，这几天会把一些学习笔记陆续上传。本人也是第一次用markdown语法写东西，排版一直搞不定段首的两个空格，先这样吧。 开摆","categories":[{"name":"个人主页","slug":"个人主页","permalink":"http://www.shelven.com/categories/%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"}],"tags":[{"name":"建站","slug":"建站","permalink":"http://www.shelven.com/tags/%E5%BB%BA%E7%AB%99/"}]}],"categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.shelven.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"转录组数据分析","slug":"转录组数据分析","permalink":"http://www.shelven.com/categories/%E8%BD%AC%E5%BD%95%E7%BB%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"编程自学","slug":"编程自学","permalink":"http://www.shelven.com/categories/%E7%BC%96%E7%A8%8B%E8%87%AA%E5%AD%A6/"},{"name":"个人主页","slug":"个人主页","permalink":"http://www.shelven.com/categories/%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"}],"tags":[{"name":"AnnotationHub","slug":"AnnotationHub","permalink":"http://www.shelven.com/tags/AnnotationHub/"},{"name":"GO/KEGG","slug":"GO-KEGG","permalink":"http://www.shelven.com/tags/GO-KEGG/"},{"name":"org.At.tair.db","slug":"org-At-tair-db","permalink":"http://www.shelven.com/tags/org-At-tair-db/"},{"name":"python","slug":"python","permalink":"http://www.shelven.com/tags/python/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://www.shelven.com/tags/ffmpeg/"},{"name":"numpy","slug":"numpy","permalink":"http://www.shelven.com/tags/numpy/"},{"name":"pillow","slug":"pillow","permalink":"http://www.shelven.com/tags/pillow/"},{"name":"R语言","slug":"R语言","permalink":"http://www.shelven.com/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"pheatmap","slug":"pheatmap","permalink":"http://www.shelven.com/tags/pheatmap/"},{"name":"SRA","slug":"SRA","permalink":"http://www.shelven.com/tags/SRA/"},{"name":"SRA Toolkit","slug":"SRA-Toolkit","permalink":"http://www.shelven.com/tags/SRA-Toolkit/"},{"name":"GEO","slug":"GEO","permalink":"http://www.shelven.com/tags/GEO/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.shelven.com/tags/%E7%88%AC%E8%99%AB/"},{"name":"vscode","slug":"vscode","permalink":"http://www.shelven.com/tags/vscode/"},{"name":"ggplot2","slug":"ggplot2","permalink":"http://www.shelven.com/tags/ggplot2/"},{"name":"ggrepel","slug":"ggrepel","permalink":"http://www.shelven.com/tags/ggrepel/"},{"name":"linux指令","slug":"linux指令","permalink":"http://www.shelven.com/tags/linux%E6%8C%87%E4%BB%A4/"},{"name":"shell脚本","slug":"shell脚本","permalink":"http://www.shelven.com/tags/shell%E8%84%9A%E6%9C%AC/"},{"name":"DESeq2","slug":"DESeq2","permalink":"http://www.shelven.com/tags/DESeq2/"},{"name":"dplyr","slug":"dplyr","permalink":"http://www.shelven.com/tags/dplyr/"},{"name":"HTseq","slug":"HTseq","permalink":"http://www.shelven.com/tags/HTseq/"},{"name":"stringtie","slug":"stringtie","permalink":"http://www.shelven.com/tags/stringtie/"},{"name":"prepDE.py3","slug":"prepDE-py3","permalink":"http://www.shelven.com/tags/prepDE-py3/"},{"name":"IGV","slug":"IGV","permalink":"http://www.shelven.com/tags/IGV/"},{"name":"samtools","slug":"samtools","permalink":"http://www.shelven.com/tags/samtools/"},{"name":"hisat2","slug":"hisat2","permalink":"http://www.shelven.com/tags/hisat2/"},{"name":"fastqc","slug":"fastqc","permalink":"http://www.shelven.com/tags/fastqc/"},{"name":"multiqc","slug":"multiqc","permalink":"http://www.shelven.com/tags/multiqc/"},{"name":"trim-galore","slug":"trim-galore","permalink":"http://www.shelven.com/tags/trim-galore/"},{"name":"建站","slug":"建站","permalink":"http://www.shelven.com/tags/%E5%BB%BA%E7%AB%99/"}]}